========== A1GymEnv-v0 ==========
Seed: 157502902
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('batch_size', 128),
             ('clip_range', 0.4),
             ('ent_coef', 0.0),
             ('env_wrapper', 'utils.wrappers.DummyWrapper'),
             ('gae_lambda', 0.9),
             ('gamma', 0.99),
             ('learning_rate', 3e-05),
             ('max_grad_norm', 0.5),
             ('n_envs', 16),
             ('n_epochs', 20),
             ('n_steps', 512),
             ('n_timesteps', 2000000.0),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs',
              'dict(log_std_init=-2, ortho_init=False, activation_fn=nn.ReLU, '
              'net_arch=[dict(pi=[256, 256], vf=[256, 256])], )'),
             ('sde_sample_freq', 4),
             ('use_sde', True),
             ('vf_coef', 0.5)])
Using 16 environments
Creating test environment
Normalization activated: {'gamma': 0.99, 'norm_reward': False}
Normalization activated: {'gamma': 0.99}
Using cuda device
Log path: logs/ppo/A1GymEnv-v0_119
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | -0.0939  |
| time/              |          |
|    fps             | 391      |
|    iterations      | 1        |
|    time_elapsed    | 20       |
|    total_timesteps | 8192     |
---------------------------------
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
Eval num_timesteps=10000, episode_reward=0.92 +/- 0.00
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 0.921       |
| time/                   |             |
|    total timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.105079085 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.14       |
|    explained_variance   | 0.00416     |
|    learning_rate        | 3e-05       |
|    loss                 | -0.181      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.142      |
|    std                  | 0.134       |
|    value_loss           | 0.155       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | -0.0885  |
| time/              |          |
|    fps             | 111      |
|    iterations      | 2        |
|    time_elapsed    | 147      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.95 +/- 0.00
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 0.946       |
| time/                   |             |
|    total timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.122751474 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.6        |
|    explained_variance   | 0.0706      |
|    learning_rate        | 3e-05       |
|    loss                 | -0.22       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.173      |
|    std                  | 0.133       |
|    value_loss           | 0.0699      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | -0.0886  |
| time/              |          |
|    fps             | 90       |
|    iterations      | 3        |
|    time_elapsed    | 272      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=30000, episode_reward=0.95 +/- 0.01
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 0.955       |
| time/                   |             |
|    total timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.107522815 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.55       |
|    explained_variance   | 0.115       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.168      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.159      |
|    std                  | 0.132       |
|    value_loss           | 0.0522      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | -0.0692  |
| time/              |          |
|    fps             | 83       |
|    iterations      | 4        |
|    time_elapsed    | 392      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=40000, episode_reward=0.90 +/- 0.06
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 0.896       |
| time/                   |             |
|    total timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.104981616 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.199       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.215      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.162      |
|    std                  | 0.131       |
|    value_loss           | 0.0449      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 256      |
|    ep_rew_mean     | -0.0637  |
| time/              |          |
|    fps             | 79       |
|    iterations      | 5        |
|    time_elapsed    | 517      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 254        |
|    ep_rew_mean          | -0.0367    |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 6          |
|    time_elapsed         | 588        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.11064194 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -7.55      |
|    explained_variance   | 0.179      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.188     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.13       |
|    value_loss           | 0.0456     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=0.79 +/- 0.07
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 0.79        |
| time/                   |             |
|    total timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.101753615 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.4         |
|    entropy_loss         | -7.65       |
|    explained_variance   | 0.292       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.2        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.159      |
|    std                  | 0.128       |
|    value_loss           | 0.0458      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | -0.0322  |
| time/              |          |
|    fps             | 80       |
|    iterations      | 7        |
|    time_elapsed    | 713      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=0.11 +/- 0.00
Episode length: 51.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51         |
|    mean_reward          | 0.106      |
| time/                   |            |
|    total timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.10124333 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.2       |
|    explained_variance   | 0.332      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.213     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.127      |
|    value_loss           | 0.045      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | -0.0117  |
| time/              |          |
|    fps             | 83       |
|    iterations      | 8        |
|    time_elapsed    | 783      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=70000, episode_reward=0.12 +/- 0.00
Episode length: 48.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48         |
|    mean_reward          | 0.118      |
| time/                   |            |
|    total timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.09367859 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.36      |
|    explained_variance   | 0.282      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.22      |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.126      |
|    value_loss           | 0.0419     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 0.00298  |
| time/              |          |
|    fps             | 85       |
|    iterations      | 9        |
|    time_elapsed    | 857      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=80000, episode_reward=0.11 +/- 0.00
Episode length: 43.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 43         |
|    mean_reward          | 0.109      |
| time/                   |            |
|    total timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.08701015 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.93      |
|    explained_variance   | 0.377      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.195     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.125      |
|    value_loss           | 0.0366     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 0.0235   |
| time/              |          |
|    fps             | 87       |
|    iterations      | 10       |
|    time_elapsed    | 935      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=90000, episode_reward=0.11 +/- 0.00
Episode length: 44.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44         |
|    mean_reward          | 0.108      |
| time/                   |            |
|    total timesteps      | 90000      |
| train/                  |            |
|    approx_kl            | 0.08779139 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.9       |
|    explained_variance   | 0.393      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.218     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.124      |
|    value_loss           | 0.0351     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 0.0346   |
| time/              |          |
|    fps             | 88       |
|    iterations      | 11       |
|    time_elapsed    | 1014     |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 0.0551     |
| time/                   |            |
|    fps                  | 90         |
|    iterations           | 12         |
|    time_elapsed         | 1090       |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.10019171 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.43      |
|    explained_variance   | 0.64       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.23      |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.123      |
|    value_loss           | 0.036      |
----------------------------------------
Eval num_timesteps=100000, episode_reward=0.09 +/- 0.00
Episode length: 40.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40         |
|    mean_reward          | 0.0948     |
| time/                   |            |
|    total timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.09456389 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.68      |
|    explained_variance   | 0.644      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.241     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.122      |
|    value_loss           | 0.0345     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 0.0603   |
| time/              |          |
|    fps             | 91       |
|    iterations      | 13       |
|    time_elapsed    | 1163     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=0.09 +/- 0.00
Episode length: 40.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40         |
|    mean_reward          | 0.0924     |
| time/                   |            |
|    total timesteps      | 110000     |
| train/                  |            |
|    approx_kl            | 0.09018497 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.66      |
|    explained_variance   | 0.629      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.234     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.122      |
|    value_loss           | 0.0376     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 0.0659   |
| time/              |          |
|    fps             | 92       |
|    iterations      | 14       |
|    time_elapsed    | 1238     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=120000, episode_reward=0.11 +/- 0.00
Episode length: 42.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42         |
|    mean_reward          | 0.115      |
| time/                   |            |
|    total timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.09449093 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10        |
|    explained_variance   | 0.684      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.206     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.121      |
|    value_loss           | 0.0347     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 0.0797   |
| time/              |          |
|    fps             | 93       |
|    iterations      | 15       |
|    time_elapsed    | 1315     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=130000, episode_reward=0.13 +/- 0.00
Episode length: 47.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47          |
|    mean_reward          | 0.129       |
| time/                   |             |
|    total timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.100044765 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.4         |
|    entropy_loss         | -10.5       |
|    explained_variance   | 0.732       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.243      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.17       |
|    std                  | 0.12        |
|    value_loss           | 0.0327      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.0895   |
| time/              |          |
|    fps             | 94       |
|    iterations      | 16       |
|    time_elapsed    | 1391     |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 118        |
|    ep_rew_mean          | 0.109      |
| time/                   |            |
|    fps                  | 95         |
|    iterations           | 17         |
|    time_elapsed         | 1465       |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.09402688 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.686      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.248     |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.119      |
|    value_loss           | 0.0318     |
----------------------------------------
Eval num_timesteps=140000, episode_reward=0.13 +/- 0.00
Episode length: 51.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51         |
|    mean_reward          | 0.133      |
| time/                   |            |
|    total timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.09717111 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.67       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.212     |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.118      |
|    value_loss           | 0.0297     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 0.112    |
| time/              |          |
|    fps             | 95       |
|    iterations      | 18       |
|    time_elapsed    | 1540     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=0.12 +/- 0.00
Episode length: 49.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49         |
|    mean_reward          | 0.119      |
| time/                   |            |
|    total timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.08836047 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.747      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.212     |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.117      |
|    value_loss           | 0.0284     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.5     |
|    ep_rew_mean     | 0.0958   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 19       |
|    time_elapsed    | 1616     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=160000, episode_reward=0.13 +/- 0.00
Episode length: 50.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | 0.128      |
| time/                   |            |
|    total timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.08521144 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.682      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.213     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.116      |
|    value_loss           | 0.0287     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.7     |
|    ep_rew_mean     | 0.105    |
| time/              |          |
|    fps             | 96       |
|    iterations      | 20       |
|    time_elapsed    | 1692     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=170000, episode_reward=0.12 +/- 0.00
Episode length: 50.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | 0.123     |
| time/                   |           |
|    total timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0953748 |
|    clip_fraction        | 0.283     |
|    clip_range           | 0.4       |
|    entropy_loss         | -11.3     |
|    explained_variance   | 0.79      |
|    learning_rate        | 3e-05     |
|    loss                 | -0.218    |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.158    |
|    std                  | 0.115     |
|    value_loss           | 0.0244    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.114    |
| time/              |          |
|    fps             | 97       |
|    iterations      | 21       |
|    time_elapsed    | 1771     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=180000, episode_reward=0.12 +/- 0.00
Episode length: 47.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47        |
|    mean_reward          | 0.116     |
| time/                   |           |
|    total timesteps      | 180000    |
| train/                  |           |
|    approx_kl            | 0.0933665 |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.4       |
|    entropy_loss         | -11.5     |
|    explained_variance   | 0.818     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.212    |
|    n_updates            | 420       |
|    policy_gradient_loss | -0.159    |
|    std                  | 0.114     |
|    value_loss           | 0.029     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 0.122    |
| time/              |          |
|    fps             | 97       |
|    iterations      | 22       |
|    time_elapsed    | 1846     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 0.12        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 23          |
|    time_elapsed         | 1917        |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.092000276 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.5       |
|    explained_variance   | 0.821       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.205      |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.165      |
|    std                  | 0.113       |
|    value_loss           | 0.0261      |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=0.14 +/- 0.00
Episode length: 53.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53         |
|    mean_reward          | 0.138      |
| time/                   |            |
|    total timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.09627505 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.8      |
|    explained_variance   | 0.833      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.186     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.113      |
|    value_loss           | 0.0256     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.3     |
|    ep_rew_mean     | 0.132    |
| time/              |          |
|    fps             | 98       |
|    iterations      | 24       |
|    time_elapsed    | 1994     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=0.14 +/- 0.00
Episode length: 52.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | 0.135       |
| time/                   |             |
|    total timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.090453215 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.9       |
|    explained_variance   | 0.841       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.211      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.157      |
|    std                  | 0.112       |
|    value_loss           | 0.0238      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | 0.127    |
| time/              |          |
|    fps             | 98       |
|    iterations      | 25       |
|    time_elapsed    | 2071     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=210000, episode_reward=0.13 +/- 0.00
Episode length: 56.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 56         |
|    mean_reward          | 0.127      |
| time/                   |            |
|    total timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.09651834 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.8      |
|    explained_variance   | 0.841      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.217     |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.111      |
|    value_loss           | 0.0287     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.4     |
|    ep_rew_mean     | 0.146    |
| time/              |          |
|    fps             | 99       |
|    iterations      | 26       |
|    time_elapsed    | 2144     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=220000, episode_reward=0.14 +/- 0.00
Episode length: 57.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 57         |
|    mean_reward          | 0.141      |
| time/                   |            |
|    total timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.08981152 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.8      |
|    explained_variance   | 0.891      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.207     |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.11       |
|    value_loss           | 0.0273     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.7     |
|    ep_rew_mean     | 0.144    |
| time/              |          |
|    fps             | 99       |
|    iterations      | 27       |
|    time_elapsed    | 2220     |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 94.2        |
|    ep_rew_mean          | 0.14        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 28          |
|    time_elapsed         | 2292        |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.090934746 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.891       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.212      |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.156      |
|    std                  | 0.109       |
|    value_loss           | 0.025       |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=0.15 +/- 0.00
Episode length: 58.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 58         |
|    mean_reward          | 0.153      |
| time/                   |            |
|    total timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.09922382 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12        |
|    explained_variance   | 0.872      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.223     |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.109      |
|    value_loss           | 0.0258     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 0.146    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 29       |
|    time_elapsed    | 2365     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=0.14 +/- 0.00
Episode length: 57.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 57         |
|    mean_reward          | 0.136      |
| time/                   |            |
|    total timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.09264919 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.9      |
|    explained_variance   | 0.884      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.204     |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.108      |
|    value_loss           | 0.0282     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.162    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 30       |
|    time_elapsed    | 2439     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=250000, episode_reward=0.15 +/- 0.00
Episode length: 59.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 59         |
|    mean_reward          | 0.154      |
| time/                   |            |
|    total timesteps      | 250000     |
| train/                  |            |
|    approx_kl            | 0.09108367 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.9      |
|    explained_variance   | 0.883      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.199     |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.108      |
|    value_loss           | 0.0268     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 31       |
|    time_elapsed    | 2511     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=260000, episode_reward=0.14 +/- 0.00
Episode length: 57.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 57         |
|    mean_reward          | 0.143      |
| time/                   |            |
|    total timesteps      | 260000     |
| train/                  |            |
|    approx_kl            | 0.09418152 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12        |
|    explained_variance   | 0.881      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.22      |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.107      |
|    value_loss           | 0.0249     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 0.172    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 32       |
|    time_elapsed    | 2586     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=270000, episode_reward=0.15 +/- 0.00
Episode length: 57.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 57         |
|    mean_reward          | 0.145      |
| time/                   |            |
|    total timesteps      | 270000     |
| train/                  |            |
|    approx_kl            | 0.09192246 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.2      |
|    explained_variance   | 0.891      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.212     |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.161     |
|    std                  | 0.106      |
|    value_loss           | 0.0266     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 0.175    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 33       |
|    time_elapsed    | 2662     |
|    total_timesteps | 270336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 109        |
|    ep_rew_mean          | 0.17       |
| time/                   |            |
|    fps                  | 101        |
|    iterations           | 34         |
|    time_elapsed         | 2735       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.09026621 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.3      |
|    explained_variance   | 0.897      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.21      |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.105      |
|    value_loss           | 0.0243     |
----------------------------------------
Eval num_timesteps=280000, episode_reward=0.17 +/- 0.00
Episode length: 62.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 62        |
|    mean_reward          | 0.169     |
| time/                   |           |
|    total timesteps      | 280000    |
| train/                  |           |
|    approx_kl            | 0.0927192 |
|    clip_fraction        | 0.271     |
|    clip_range           | 0.4       |
|    entropy_loss         | -12.4     |
|    explained_variance   | 0.904     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.225    |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.105     |
|    value_loss           | 0.0232    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.185    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 35       |
|    time_elapsed    | 2811     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=0.17 +/- 0.00
Episode length: 62.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 62        |
|    mean_reward          | 0.168     |
| time/                   |           |
|    total timesteps      | 290000    |
| train/                  |           |
|    approx_kl            | 0.0913171 |
|    clip_fraction        | 0.27      |
|    clip_range           | 0.4       |
|    entropy_loss         | -12.3     |
|    explained_variance   | 0.879     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.207    |
|    n_updates            | 700       |
|    policy_gradient_loss | -0.161    |
|    std                  | 0.104     |
|    value_loss           | 0.0282    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.184    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 36       |
|    time_elapsed    | 2887     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=300000, episode_reward=0.17 +/- 0.00
Episode length: 64.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 64         |
|    mean_reward          | 0.166      |
| time/                   |            |
|    total timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.09280442 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.4      |
|    explained_variance   | 0.896      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.224     |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.103      |
|    value_loss           | 0.0293     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 0.205    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 37       |
|    time_elapsed    | 2962     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=310000, episode_reward=0.18 +/- 0.00
Episode length: 67.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 67         |
|    mean_reward          | 0.18       |
| time/                   |            |
|    total timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.09384084 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.2      |
|    explained_variance   | 0.895      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.227     |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.103      |
|    value_loss           | 0.0274     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 0.204    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 38       |
|    time_elapsed    | 3040     |
|    total_timesteps | 311296   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 0.212       |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 39          |
|    time_elapsed         | 3117        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.095281415 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.5       |
|    explained_variance   | 0.891       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.21       |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.166      |
|    std                  | 0.102       |
|    value_loss           | 0.0291      |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=0.22 +/- 0.00
Episode length: 73.80 +/- 0.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 73.8       |
|    mean_reward          | 0.216      |
| time/                   |            |
|    total timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.09437866 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.4      |
|    explained_variance   | 0.892      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.227     |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.166     |
|    std                  | 0.101      |
|    value_loss           | 0.0272     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 0.238    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 40       |
|    time_elapsed    | 3194     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=0.22 +/- 0.00
Episode length: 80.40 +/- 0.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 80.4       |
|    mean_reward          | 0.219      |
| time/                   |            |
|    total timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.09689769 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.894      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.194     |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.101      |
|    value_loss           | 0.027      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 0.235    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 41       |
|    time_elapsed    | 3272     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=340000, episode_reward=0.24 +/- 0.00
Episode length: 82.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 82         |
|    mean_reward          | 0.237      |
| time/                   |            |
|    total timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.10068716 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.893      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.211     |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.1        |
|    value_loss           | 0.0276     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 0.272    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 42       |
|    time_elapsed    | 3349     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=350000, episode_reward=0.30 +/- 0.00
Episode length: 101.80 +/- 1.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.10506899 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.6      |
|    explained_variance   | 0.901      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.231     |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.177     |
|    std                  | 0.0995     |
|    value_loss           | 0.0276     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 0.302    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 43       |
|    time_elapsed    | 3428     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=360000, episode_reward=0.31 +/- 0.01
Episode length: 102.80 +/- 2.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 0.314       |
| time/                   |             |
|    total timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.095038526 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.878       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.196      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.167      |
|    std                  | 0.0989      |
|    value_loss           | 0.0311      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 0.342    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 44       |
|    time_elapsed    | 3506     |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 0.335      |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 45         |
|    time_elapsed         | 3583       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.09908143 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.5      |
|    explained_variance   | 0.89       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.205     |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.174     |
|    std                  | 0.0983     |
|    value_loss           | 0.0312     |
----------------------------------------
Eval num_timesteps=370000, episode_reward=0.86 +/- 0.12
Episode length: 217.40 +/- 18.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 0.862      |
| time/                   |            |
|    total timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.09751243 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.6      |
|    explained_variance   | 0.895      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.2       |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.0977     |
|    value_loss           | 0.0314     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 0.377    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 46       |
|    time_elapsed    | 3670     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=0.81 +/- 0.29
Episode length: 220.00 +/- 89.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 220        |
|    mean_reward          | 0.807      |
| time/                   |            |
|    total timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.09875734 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.6      |
|    explained_variance   | 0.889      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.222     |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.097      |
|    value_loss           | 0.0332     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 0.372    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 47       |
|    time_elapsed    | 3752     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=390000, episode_reward=0.71 +/- 0.10
Episode length: 198.60 +/- 28.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 0.707       |
| time/                   |             |
|    total timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.094873205 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.8       |
|    explained_variance   | 0.902       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.212      |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.17       |
|    std                  | 0.0964      |
|    value_loss           | 0.0315      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 0.337    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 48       |
|    time_elapsed    | 3835     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=400000, episode_reward=0.66 +/- 0.01
Episode length: 169.40 +/- 5.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 169        |
|    mean_reward          | 0.657      |
| time/                   |            |
|    total timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.09580246 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.8      |
|    explained_variance   | 0.901      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.211     |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.0958     |
|    value_loss           | 0.0311     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 0.361    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 49       |
|    time_elapsed    | 3917     |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 0.39        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 50          |
|    time_elapsed         | 3996        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.094997495 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.5       |
|    explained_variance   | 0.877       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.212      |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.168      |
|    std                  | 0.0953      |
|    value_loss           | 0.0362      |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=0.70 +/- 0.18
Episode length: 225.40 +/- 59.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 0.701      |
| time/                   |            |
|    total timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.10177721 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.8      |
|    explained_variance   | 0.902      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.224     |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.0948     |
|    value_loss           | 0.0331     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 0.435    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 51       |
|    time_elapsed    | 4081     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=0.88 +/- 0.27
Episode length: 374.60 +/- 99.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 375        |
|    mean_reward          | 0.877      |
| time/                   |            |
|    total timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.09586938 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.6      |
|    explained_variance   | 0.897      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.247     |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.0944     |
|    value_loss           | 0.0309     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | 0.475    |
| time/              |          |
|    fps             | 102      |
|    iterations      | 52       |
|    time_elapsed    | 4172     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=0.47 +/- 0.00
Episode length: 173.00 +/- 1.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 0.467      |
| time/                   |            |
|    total timesteps      | 430000     |
| train/                  |            |
|    approx_kl            | 0.10185374 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.884      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.206     |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.172     |
|    std                  | 0.0937     |
|    value_loss           | 0.0372     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 256      |
|    ep_rew_mean     | 0.503    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 53       |
|    time_elapsed    | 4257     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=440000, episode_reward=0.73 +/- 0.05
Episode length: 194.20 +/- 20.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 194        |
|    mean_reward          | 0.729      |
| time/                   |            |
|    total timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.09216339 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.8      |
|    explained_variance   | 0.866      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.222     |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.164     |
|    std                  | 0.0932     |
|    value_loss           | 0.0374     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 0.546    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 54       |
|    time_elapsed    | 4341     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=450000, episode_reward=0.63 +/- 0.05
Episode length: 169.20 +/- 15.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 169        |
|    mean_reward          | 0.629      |
| time/                   |            |
|    total timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.10057922 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.8      |
|    explained_variance   | 0.899      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.217     |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.0926     |
|    value_loss           | 0.0295     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 283      |
|    ep_rew_mean     | 0.569    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 55       |
|    time_elapsed    | 4424     |
|    total_timesteps | 450560   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 316        |
|    ep_rew_mean          | 0.654      |
| time/                   |            |
|    fps                  | 101        |
|    iterations           | 56         |
|    time_elapsed         | 4504       |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.10484369 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.894      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.2       |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.0924     |
|    value_loss           | 0.0285     |
----------------------------------------
Eval num_timesteps=460000, episode_reward=2.87 +/- 1.91
Episode length: 860.00 +/- 611.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 860        |
|    mean_reward          | 2.87       |
| time/                   |            |
|    total timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.09966248 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.909      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.215     |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.0918     |
|    value_loss           | 0.0298     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | 0.703    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 57       |
|    time_elapsed    | 4609     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=0.58 +/- 0.01
Episode length: 195.80 +/- 3.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | 0.579       |
| time/                   |             |
|    total timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.102233514 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.8       |
|    explained_variance   | 0.868       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.211      |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.169      |
|    std                  | 0.0914      |
|    value_loss           | 0.033       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | 0.762    |
| time/              |          |
|    fps             | 101      |
|    iterations      | 58       |
|    time_elapsed    | 4692     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=480000, episode_reward=4.90 +/- 2.37
Episode length: 1181.20 +/- 415.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.18e+03   |
|    mean_reward          | 4.9        |
| time/                   |            |
|    total timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.09565213 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.839      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.197     |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.168     |
|    std                  | 0.0909     |
|    value_loss           | 0.0296     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 402      |
|    ep_rew_mean     | 0.861    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 59       |
|    time_elapsed    | 4812     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=490000, episode_reward=0.92 +/- 0.11
Episode length: 259.20 +/- 28.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 259        |
|    mean_reward          | 0.923      |
| time/                   |            |
|    total timesteps      | 490000     |
| train/                  |            |
|    approx_kl            | 0.10188474 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.894      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.215     |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.0903     |
|    value_loss           | 0.0295     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | 0.833    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 60       |
|    time_elapsed    | 4902     |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 394        |
|    ep_rew_mean          | 0.866      |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 61         |
|    time_elapsed         | 4983       |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.10318647 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.9        |
|    learning_rate        | 3e-05      |
|    loss                 | -0.224     |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.169     |
|    std                  | 0.0897     |
|    value_loss           | 0.0284     |
----------------------------------------
Eval num_timesteps=500000, episode_reward=1.26 +/- 0.44
Episode length: 305.80 +/- 116.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 306         |
|    mean_reward          | 1.26        |
| time/                   |             |
|    total timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.099461704 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13         |
|    explained_variance   | 0.853       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.262      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.169      |
|    std                  | 0.0892      |
|    value_loss           | 0.0354      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 389      |
|    ep_rew_mean     | 0.879    |
| time/              |          |
|    fps             | 100      |
|    iterations      | 62       |
|    time_elapsed    | 5076     |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=4.25 +/- 3.82
Episode length: 877.20 +/- 604.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 877        |
|    mean_reward          | 4.25       |
| time/                   |            |
|    total timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.10099212 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.9      |
|    explained_variance   | 0.855      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.228     |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.0889     |
|    value_loss           | 0.0315     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 406      |
|    ep_rew_mean     | 0.948    |
| time/              |          |
|    fps             | 99       |
|    iterations      | 63       |
|    time_elapsed    | 5188     |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=2.80 +/- 3.92
Episode length: 529.60 +/- 643.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 530        |
|    mean_reward          | 2.8        |
| time/                   |            |
|    total timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.10798186 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.9      |
|    explained_variance   | 0.881      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.21      |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.176     |
|    std                  | 0.0886     |
|    value_loss           | 0.0326     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 385      |
|    ep_rew_mean     | 0.937    |
| time/              |          |
|    fps             | 99       |
|    iterations      | 64       |
|    time_elapsed    | 5286     |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=530000, episode_reward=3.55 +/- 4.06
Episode length: 697.80 +/- 660.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 698        |
|    mean_reward          | 3.55       |
| time/                   |            |
|    total timesteps      | 530000     |
| train/                  |            |
|    approx_kl            | 0.10620734 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.855      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.221     |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.171     |
|    std                  | 0.088      |
|    value_loss           | 0.0324     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 419      |
|    ep_rew_mean     | 1.03     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 65       |
|    time_elapsed    | 5390     |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=540000, episode_reward=2.40 +/- 0.98
Episode length: 604.00 +/- 236.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 604        |
|    mean_reward          | 2.4        |
| time/                   |            |
|    total timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.10047573 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.904      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.218     |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.0874     |
|    value_loss           | 0.0317     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | 1.05     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 66       |
|    time_elapsed    | 5487     |
|    total_timesteps | 540672   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 431         |
|    ep_rew_mean          | 1.08        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 67          |
|    time_elapsed         | 5568        |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.096404165 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.2       |
|    explained_variance   | 0.892       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.215      |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.164      |
|    std                  | 0.087       |
|    value_loss           | 0.028       |
-----------------------------------------
Eval num_timesteps=550000, episode_reward=7.87 +/- 3.80
Episode length: 1508.20 +/- 611.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.51e+03  |
|    mean_reward          | 7.87      |
| time/                   |           |
|    total timesteps      | 550000    |
| train/                  |           |
|    approx_kl            | 0.1069369 |
|    clip_fraction        | 0.32      |
|    clip_range           | 0.4       |
|    entropy_loss         | -13.2     |
|    explained_variance   | 0.91      |
|    learning_rate        | 3e-05     |
|    loss                 | -0.23     |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.17     |
|    std                  | 0.0868    |
|    value_loss           | 0.0251    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 1.08     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 68       |
|    time_elapsed    | 5696     |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=7.68 +/- 4.69
Episode length: 1405.80 +/- 721.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.41e+03   |
|    mean_reward          | 7.68       |
| time/                   |            |
|    total timesteps      | 560000     |
| train/                  |            |
|    approx_kl            | 0.10319227 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.883      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.201     |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.0863     |
|    value_loss           | 0.0299     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 408      |
|    ep_rew_mean     | 1.04     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 69       |
|    time_elapsed    | 5823     |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=570000, episode_reward=8.42 +/- 5.61
Episode length: 1371.00 +/- 770.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.37e+03    |
|    mean_reward          | 8.42        |
| time/                   |             |
|    total timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.099857226 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.2       |
|    explained_variance   | 0.879       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.237      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.161      |
|    std                  | 0.0857      |
|    value_loss           | 0.0312      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 409      |
|    ep_rew_mean     | 1.06     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 70       |
|    time_elapsed    | 5948     |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=580000, episode_reward=6.15 +/- 5.53
Episode length: 1039.80 +/- 780.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.04e+03   |
|    mean_reward          | 6.15       |
| time/                   |            |
|    total timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.09669037 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.883      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.229     |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.0854     |
|    value_loss           | 0.0287     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 431      |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 71       |
|    time_elapsed    | 6060     |
|    total_timesteps | 581632   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 456        |
|    ep_rew_mean          | 1.23       |
| time/                   |            |
|    fps                  | 96         |
|    iterations           | 72         |
|    time_elapsed         | 6139       |
|    total_timesteps      | 589824     |
| train/                  |            |
|    approx_kl            | 0.09795909 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.884      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.208     |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.0849     |
|    value_loss           | 0.0311     |
----------------------------------------
Eval num_timesteps=590000, episode_reward=4.35 +/- 4.99
Episode length: 803.20 +/- 659.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 803        |
|    mean_reward          | 4.35       |
| time/                   |            |
|    total timesteps      | 590000     |
| train/                  |            |
|    approx_kl            | 0.10253684 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.816      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.236     |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.165     |
|    std                  | 0.0846     |
|    value_loss           | 0.0292     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | 1.33     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 73       |
|    time_elapsed    | 6245     |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=7.10 +/- 5.56
Episode length: 1199.60 +/- 675.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.2e+03    |
|    mean_reward          | 7.1        |
| time/                   |            |
|    total timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.10413649 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.863      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.217     |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.17      |
|    std                  | 0.0841     |
|    value_loss           | 0.0267     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | 1.39     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 74       |
|    time_elapsed    | 6361     |
|    total_timesteps | 606208   |
---------------------------------
Eval num_timesteps=610000, episode_reward=3.29 +/- 4.54
Episode length: 619.80 +/- 692.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 620        |
|    mean_reward          | 3.29       |
| time/                   |            |
|    total timesteps      | 610000     |
| train/                  |            |
|    approx_kl            | 0.10610917 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.88       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.23      |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.0836     |
|    value_loss           | 0.0299     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | 1.43     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 75       |
|    time_elapsed    | 6457     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=620000, episode_reward=0.63 +/- 0.01
Episode length: 168.60 +/- 3.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 169        |
|    mean_reward          | 0.627      |
| time/                   |            |
|    total timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.09946336 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.912      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.204     |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.167     |
|    std                  | 0.0833     |
|    value_loss           | 0.0291     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | 1.42     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 76       |
|    time_elapsed    | 6534     |
|    total_timesteps | 622592   |
---------------------------------
Eval num_timesteps=630000, episode_reward=4.54 +/- 4.47
Episode length: 835.80 +/- 607.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 836        |
|    mean_reward          | 4.54       |
| time/                   |            |
|    total timesteps      | 630000     |
| train/                  |            |
|    approx_kl            | 0.10119319 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.876      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.202     |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.0828     |
|    value_loss           | 0.0286     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | 1.46     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 77       |
|    time_elapsed    | 6638     |
|    total_timesteps | 630784   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 523         |
|    ep_rew_mean          | 1.54        |
| time/                   |             |
|    fps                  | 95          |
|    iterations           | 78          |
|    time_elapsed         | 6716        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.100439474 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.2       |
|    explained_variance   | 0.866       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.179      |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.164      |
|    std                  | 0.0825      |
|    value_loss           | 0.0219      |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=6.84 +/- 5.46
Episode length: 1140.60 +/- 724.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.14e+03  |
|    mean_reward          | 6.84      |
| time/                   |           |
|    total timesteps      | 640000    |
| train/                  |           |
|    approx_kl            | 0.1031985 |
|    clip_fraction        | 0.304     |
|    clip_range           | 0.4       |
|    entropy_loss         | -13.4     |
|    explained_variance   | 0.892     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.214    |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.164    |
|    std                  | 0.0821    |
|    value_loss           | 0.0255    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 527      |
|    ep_rew_mean     | 1.59     |
| time/              |          |
|    fps             | 94       |
|    iterations      | 79       |
|    time_elapsed    | 6833     |
|    total_timesteps | 647168   |
---------------------------------
Eval num_timesteps=650000, episode_reward=1.71 +/- 0.61
Episode length: 389.00 +/- 125.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 389        |
|    mean_reward          | 1.71       |
| time/                   |            |
|    total timesteps      | 650000     |
| train/                  |            |
|    approx_kl            | 0.10739425 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.888      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.221     |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.163     |
|    std                  | 0.0817     |
|    value_loss           | 0.0269     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | 1.62     |
| time/              |          |
|    fps             | 94       |
|    iterations      | 80       |
|    time_elapsed    | 6926     |
|    total_timesteps | 655360   |
---------------------------------
Eval num_timesteps=660000, episode_reward=13.77 +/- 4.58
Episode length: 1793.40 +/- 413.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.79e+03   |
|    mean_reward          | 13.8       |
| time/                   |            |
|    total timesteps      | 660000     |
| train/                  |            |
|    approx_kl            | 0.10426709 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.855      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.205     |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.0812     |
|    value_loss           | 0.0257     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 530      |
|    ep_rew_mean     | 1.68     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 81       |
|    time_elapsed    | 7063     |
|    total_timesteps | 663552   |
---------------------------------
Eval num_timesteps=670000, episode_reward=13.43 +/- 4.35
Episode length: 1796.00 +/- 408.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.8e+03    |
|    mean_reward          | 13.4       |
| time/                   |            |
|    total timesteps      | 670000     |
| train/                  |            |
|    approx_kl            | 0.09852581 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.884      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.226     |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.0808     |
|    value_loss           | 0.0259     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 587      |
|    ep_rew_mean     | 1.94     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 82       |
|    time_elapsed    | 7201     |
|    total_timesteps | 671744   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 588         |
|    ep_rew_mean          | 1.98        |
| time/                   |             |
|    fps                  | 93          |
|    iterations           | 83          |
|    time_elapsed         | 7281        |
|    total_timesteps      | 679936      |
| train/                  |             |
|    approx_kl            | 0.103991695 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.6       |
|    explained_variance   | 0.906       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.237      |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.164      |
|    std                  | 0.0805      |
|    value_loss           | 0.0271      |
-----------------------------------------
Eval num_timesteps=680000, episode_reward=16.19 +/- 0.32
Episode length: 2000.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2e+03     |
|    mean_reward          | 16.2      |
| time/                   |           |
|    total timesteps      | 680000    |
| train/                  |           |
|    approx_kl            | 0.0966347 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.4       |
|    entropy_loss         | -13.4     |
|    explained_variance   | 0.901     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.186    |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.158    |
|    std                  | 0.0801    |
|    value_loss           | 0.0281    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 556      |
|    ep_rew_mean     | 1.92     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 84       |
|    time_elapsed    | 7428     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=690000, episode_reward=13.84 +/- 5.54
Episode length: 1709.80 +/- 580.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.71e+03    |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total timesteps      | 690000      |
| train/                  |             |
|    approx_kl            | 0.096545786 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.3       |
|    explained_variance   | 0.852       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.196      |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.159      |
|    std                  | 0.0797      |
|    value_loss           | 0.0293      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 569      |
|    ep_rew_mean     | 2.02     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 85       |
|    time_elapsed    | 7560     |
|    total_timesteps | 696320   |
---------------------------------
Eval num_timesteps=700000, episode_reward=3.27 +/- 1.43
Episode length: 664.40 +/- 292.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 664        |
|    mean_reward          | 3.27       |
| time/                   |            |
|    total timesteps      | 700000     |
| train/                  |            |
|    approx_kl            | 0.10182226 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.866      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.227     |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.154     |
|    std                  | 0.0793     |
|    value_loss           | 0.03       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 605      |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 91       |
|    iterations      | 86       |
|    time_elapsed    | 7662     |
|    total_timesteps | 704512   |
---------------------------------
Eval num_timesteps=710000, episode_reward=10.74 +/- 6.44
Episode length: 1426.20 +/- 702.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.43e+03   |
|    mean_reward          | 10.7       |
| time/                   |            |
|    total timesteps      | 710000     |
| train/                  |            |
|    approx_kl            | 0.09651391 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.88       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.201     |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.156     |
|    std                  | 0.0789     |
|    value_loss           | 0.0335     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 602      |
|    ep_rew_mean     | 2.23     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 87       |
|    time_elapsed    | 7791     |
|    total_timesteps | 712704   |
---------------------------------
Eval num_timesteps=720000, episode_reward=13.93 +/- 4.29
Episode length: 1825.80 +/- 348.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.83e+03   |
|    mean_reward          | 13.9       |
| time/                   |            |
|    total timesteps      | 720000     |
| train/                  |            |
|    approx_kl            | 0.09972073 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.883      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.213     |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.16      |
|    std                  | 0.0785     |
|    value_loss           | 0.0278     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 603      |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 88       |
|    time_elapsed    | 7933     |
|    total_timesteps | 720896   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 621        |
|    ep_rew_mean          | 2.37       |
| time/                   |            |
|    fps                  | 90         |
|    iterations           | 89         |
|    time_elapsed         | 8013       |
|    total_timesteps      | 729088     |
| train/                  |            |
|    approx_kl            | 0.09577416 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.894      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.197     |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.0781     |
|    value_loss           | 0.0236     |
----------------------------------------
Eval num_timesteps=730000, episode_reward=14.56 +/- 5.04
Episode length: 1773.40 +/- 453.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.77e+03    |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total timesteps      | 730000      |
| train/                  |             |
|    approx_kl            | 0.094754934 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.4       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.216      |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.153      |
|    std                  | 0.0777      |
|    value_loss           | 0.0257      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 608      |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 90       |
|    time_elapsed    | 8151     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=740000, episode_reward=14.93 +/- 0.67
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 14.9       |
| time/                   |            |
|    total timesteps      | 740000     |
| train/                  |            |
|    approx_kl            | 0.09040721 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.863      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.197     |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.0773     |
|    value_loss           | 0.0234     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 635      |
|    ep_rew_mean     | 2.46     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 91       |
|    time_elapsed    | 8298     |
|    total_timesteps | 745472   |
---------------------------------
Eval num_timesteps=750000, episode_reward=14.09 +/- 0.93
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 14.1       |
| time/                   |            |
|    total timesteps      | 750000     |
| train/                  |            |
|    approx_kl            | 0.10028066 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.848      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.207     |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.151     |
|    std                  | 0.077      |
|    value_loss           | 0.0274     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 671      |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 92       |
|    time_elapsed    | 8439     |
|    total_timesteps | 753664   |
---------------------------------
Eval num_timesteps=760000, episode_reward=14.53 +/- 1.07
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 14.5       |
| time/                   |            |
|    total timesteps      | 760000     |
| train/                  |            |
|    approx_kl            | 0.10066596 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.915      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.204     |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.0766     |
|    value_loss           | 0.0214     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 676      |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 93       |
|    time_elapsed    | 8583     |
|    total_timesteps | 761856   |
---------------------------------
Eval num_timesteps=770000, episode_reward=13.03 +/- 4.98
Episode length: 1730.80 +/- 520.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.73e+03   |
|    mean_reward          | 13         |
| time/                   |            |
|    total timesteps      | 770000     |
| train/                  |            |
|    approx_kl            | 0.09968418 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.887      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.201     |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.0762     |
|    value_loss           | 0.0263     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 700      |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 94       |
|    time_elapsed    | 8713     |
|    total_timesteps | 770048   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 679        |
|    ep_rew_mean          | 2.75       |
| time/                   |            |
|    fps                  | 88         |
|    iterations           | 95         |
|    time_elapsed         | 8792       |
|    total_timesteps      | 778240     |
| train/                  |            |
|    approx_kl            | 0.10243184 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.901      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.209     |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.157     |
|    std                  | 0.0758     |
|    value_loss           | 0.0233     |
----------------------------------------
Eval num_timesteps=780000, episode_reward=15.78 +/- 0.26
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 15.8       |
| time/                   |            |
|    total timesteps      | 780000     |
| train/                  |            |
|    approx_kl            | 0.10117356 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.917      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.187     |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.159     |
|    std                  | 0.0754     |
|    value_loss           | 0.0286     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 714      |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 96       |
|    time_elapsed    | 8937     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=790000, episode_reward=16.64 +/- 0.81
Episode length: 1993.00 +/- 14.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 16.6       |
| time/                   |            |
|    total timesteps      | 790000     |
| train/                  |            |
|    approx_kl            | 0.10181939 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.892      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.213     |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.155     |
|    std                  | 0.0752     |
|    value_loss           | 0.0258     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 734      |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 97       |
|    time_elapsed    | 9087     |
|    total_timesteps | 794624   |
---------------------------------
Eval num_timesteps=800000, episode_reward=16.06 +/- 1.14
Episode length: 1995.60 +/- 8.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 16.1       |
| time/                   |            |
|    total timesteps      | 800000     |
| train/                  |            |
|    approx_kl            | 0.09786348 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.896      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.175     |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.0748     |
|    value_loss           | 0.0237     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 750      |
|    ep_rew_mean     | 3.22     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 98       |
|    time_elapsed    | 9235     |
|    total_timesteps | 802816   |
---------------------------------
Eval num_timesteps=810000, episode_reward=16.95 +/- 1.06
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 17         |
| time/                   |            |
|    total timesteps      | 810000     |
| train/                  |            |
|    approx_kl            | 0.09967575 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.892      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.186     |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.148     |
|    std                  | 0.0744     |
|    value_loss           | 0.026      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 765      |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 99       |
|    time_elapsed    | 9383     |
|    total_timesteps | 811008   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 792        |
|    ep_rew_mean          | 3.52       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 100        |
|    time_elapsed         | 9461       |
|    total_timesteps      | 819200     |
| train/                  |            |
|    approx_kl            | 0.09019263 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.3      |
|    explained_variance   | 0.893      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.173     |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.14      |
|    std                  | 0.0741     |
|    value_loss           | 0.0216     |
----------------------------------------
Eval num_timesteps=820000, episode_reward=17.95 +/- 0.41
Episode length: 2000.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 2e+03     |
|    mean_reward          | 18        |
| time/                   |           |
|    total timesteps      | 820000    |
| train/                  |           |
|    approx_kl            | 0.1053303 |
|    clip_fraction        | 0.305     |
|    clip_range           | 0.4       |
|    entropy_loss         | -13.1     |
|    explained_variance   | 0.911     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.187    |
|    n_updates            | 2000      |
|    policy_gradient_loss | -0.159    |
|    std                  | 0.0737    |
|    value_loss           | 0.0277    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 798      |
|    ep_rew_mean     | 3.59     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 101      |
|    time_elapsed    | 9602     |
|    total_timesteps | 827392   |
---------------------------------
Eval num_timesteps=830000, episode_reward=20.57 +/- 0.79
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 20.6        |
| time/                   |             |
|    total timesteps      | 830000      |
| train/                  |             |
|    approx_kl            | 0.100631475 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.2       |
|    explained_variance   | 0.922       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.194      |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.151      |
|    std                  | 0.0733      |
|    value_loss           | 0.0235      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 817      |
|    ep_rew_mean     | 3.79     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 102      |
|    time_elapsed    | 9738     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=840000, episode_reward=17.80 +/- 0.87
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 17.8       |
| time/                   |            |
|    total timesteps      | 840000     |
| train/                  |            |
|    approx_kl            | 0.10431121 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.91       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.151     |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.0731     |
|    value_loss           | 0.0276     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 860      |
|    ep_rew_mean     | 4.11     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 103      |
|    time_elapsed    | 9881     |
|    total_timesteps | 843776   |
---------------------------------
Eval num_timesteps=850000, episode_reward=17.83 +/- 0.51
Episode length: 1991.20 +/- 17.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 17.8       |
| time/                   |            |
|    total timesteps      | 850000     |
| train/                  |            |
|    approx_kl            | 0.09804723 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.2      |
|    explained_variance   | 0.929      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.206     |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.0727     |
|    value_loss           | 0.0183     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 870      |
|    ep_rew_mean     | 4.22     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 104      |
|    time_elapsed    | 10031    |
|    total_timesteps | 851968   |
---------------------------------
Eval num_timesteps=860000, episode_reward=13.94 +/- 1.03
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 13.9       |
| time/                   |            |
|    total timesteps      | 860000     |
| train/                  |            |
|    approx_kl            | 0.10322042 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13.1      |
|    explained_variance   | 0.897      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.168     |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.147     |
|    std                  | 0.0723     |
|    value_loss           | 0.0236     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 870      |
|    ep_rew_mean     | 4.23     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 105      |
|    time_elapsed    | 10178    |
|    total_timesteps | 860160   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 903        |
|    ep_rew_mean          | 4.56       |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 106        |
|    time_elapsed         | 10255      |
|    total_timesteps      | 868352     |
| train/                  |            |
|    approx_kl            | 0.10471876 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.914      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.179     |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.0719     |
|    value_loss           | 0.024      |
----------------------------------------
Eval num_timesteps=870000, episode_reward=16.08 +/- 0.41
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 16.1       |
| time/                   |            |
|    total timesteps      | 870000     |
| train/                  |            |
|    approx_kl            | 0.10722489 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.924      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.194     |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.152     |
|    std                  | 0.0716     |
|    value_loss           | 0.0341     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 866      |
|    ep_rew_mean     | 4.44     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 107      |
|    time_elapsed    | 10401    |
|    total_timesteps | 876544   |
---------------------------------
Eval num_timesteps=880000, episode_reward=14.34 +/- 0.84
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.099452615 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13         |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.201      |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.148      |
|    std                  | 0.0713      |
|    value_loss           | 0.0286      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 849      |
|    ep_rew_mean     | 4.5      |
| time/              |          |
|    fps             | 83       |
|    iterations      | 108      |
|    time_elapsed    | 10547    |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=890000, episode_reward=13.57 +/- 0.33
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 13.6        |
| time/                   |             |
|    total timesteps      | 890000      |
| train/                  |             |
|    approx_kl            | 0.098313555 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.4         |
|    entropy_loss         | -13.1       |
|    explained_variance   | 0.917       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.193      |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.147      |
|    std                  | 0.0709      |
|    value_loss           | 0.024       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 847      |
|    ep_rew_mean     | 4.56     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 109      |
|    time_elapsed    | 10695    |
|    total_timesteps | 892928   |
---------------------------------
Eval num_timesteps=900000, episode_reward=13.85 +/- 0.40
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 13.8       |
| time/                   |            |
|    total timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.09968464 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.4        |
|    entropy_loss         | -13        |
|    explained_variance   | 0.936      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.183     |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.0706     |
|    value_loss           | 0.0248     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 871      |
|    ep_rew_mean     | 4.78     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 110      |
|    time_elapsed    | 10833    |
|    total_timesteps | 901120   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 880        |
|    ep_rew_mean          | 4.93       |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 111        |
|    time_elapsed         | 10910      |
|    total_timesteps      | 909312     |
| train/                  |            |
|    approx_kl            | 0.09621288 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.9      |
|    explained_variance   | 0.935      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.173     |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.0702     |
|    value_loss           | 0.027      |
----------------------------------------
Eval num_timesteps=910000, episode_reward=10.60 +/- 4.26
Episode length: 1667.40 +/- 665.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.67e+03  |
|    mean_reward          | 10.6      |
| time/                   |           |
|    total timesteps      | 910000    |
| train/                  |           |
|    approx_kl            | 0.1028073 |
|    clip_fraction        | 0.292     |
|    clip_range           | 0.4       |
|    entropy_loss         | -12.7     |
|    explained_variance   | 0.932     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.181    |
|    n_updates            | 2220      |
|    policy_gradient_loss | -0.148    |
|    std                  | 0.0699    |
|    value_loss           | 0.0325    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 909      |
|    ep_rew_mean     | 5.27     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 112      |
|    time_elapsed    | 11040    |
|    total_timesteps | 917504   |
---------------------------------
Eval num_timesteps=920000, episode_reward=12.85 +/- 0.13
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 12.9       |
| time/                   |            |
|    total timesteps      | 920000     |
| train/                  |            |
|    approx_kl            | 0.11295499 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.93       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.192     |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.162     |
|    std                  | 0.0695     |
|    value_loss           | 0.0458     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 939      |
|    ep_rew_mean     | 5.55     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 113      |
|    time_elapsed    | 11183    |
|    total_timesteps | 925696   |
---------------------------------
Eval num_timesteps=930000, episode_reward=12.72 +/- 0.25
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total timesteps      | 930000     |
| train/                  |            |
|    approx_kl            | 0.10163838 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.8      |
|    explained_variance   | 0.928      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.212     |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.142     |
|    std                  | 0.0692     |
|    value_loss           | 0.0311     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 912      |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 114      |
|    time_elapsed    | 11326    |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=940000, episode_reward=13.74 +/- 0.14
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 13.7       |
| time/                   |            |
|    total timesteps      | 940000     |
| train/                  |            |
|    approx_kl            | 0.10052939 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.7      |
|    explained_variance   | 0.929      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.177     |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.146     |
|    std                  | 0.0689     |
|    value_loss           | 0.0306     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 918      |
|    ep_rew_mean     | 5.55     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 115      |
|    time_elapsed    | 11470    |
|    total_timesteps | 942080   |
---------------------------------
Eval num_timesteps=950000, episode_reward=14.20 +/- 0.17
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 14.2       |
| time/                   |            |
|    total timesteps      | 950000     |
| train/                  |            |
|    approx_kl            | 0.10527614 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.5      |
|    explained_variance   | 0.939      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.196     |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.149     |
|    std                  | 0.0687     |
|    value_loss           | 0.0313     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 942      |
|    ep_rew_mean     | 5.78     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 116      |
|    time_elapsed    | 11620    |
|    total_timesteps | 950272   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 945         |
|    ep_rew_mean          | 5.93        |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 117         |
|    time_elapsed         | 11692       |
|    total_timesteps      | 958464      |
| train/                  |             |
|    approx_kl            | 0.104433626 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.177      |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.148      |
|    std                  | 0.0684      |
|    value_loss           | 0.0324      |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=14.49 +/- 0.18
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 14.5        |
| time/                   |             |
|    total timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.107714176 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.207      |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.153      |
|    std                  | 0.068       |
|    value_loss           | 0.0289      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 959      |
|    ep_rew_mean     | 6.05     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 118      |
|    time_elapsed    | 11831    |
|    total_timesteps | 966656   |
---------------------------------
Eval num_timesteps=970000, episode_reward=13.78 +/- 0.20
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total timesteps      | 970000      |
| train/                  |             |
|    approx_kl            | 0.107504874 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.191      |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.14       |
|    std                  | 0.0677      |
|    value_loss           | 0.0347      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 6.44     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 119      |
|    time_elapsed    | 11978    |
|    total_timesteps | 974848   |
---------------------------------
Eval num_timesteps=980000, episode_reward=14.67 +/- 0.55
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 14.7       |
| time/                   |            |
|    total timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.10147703 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.6      |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.176     |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.0673     |
|    value_loss           | 0.0414     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 6.82     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 120      |
|    time_elapsed    | 12125    |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=990000, episode_reward=15.44 +/- 1.39
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total timesteps      | 990000      |
| train/                  |             |
|    approx_kl            | 0.102485895 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.181      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.135      |
|    std                  | 0.067       |
|    value_loss           | 0.0386      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | 6.92     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 121      |
|    time_elapsed    | 12265    |
|    total_timesteps | 991232   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.07e+03    |
|    ep_rew_mean          | 7.33        |
| time/                   |             |
|    fps                  | 80          |
|    iterations           | 122         |
|    time_elapsed         | 12343       |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.103909224 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.939       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.138      |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.137      |
|    std                  | 0.0668      |
|    value_loss           | 0.0373      |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=18.52 +/- 2.21
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 18.5       |
| time/                   |            |
|    total timesteps      | 1000000    |
| train/                  |            |
|    approx_kl            | 0.10291233 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.4      |
|    explained_variance   | 0.947      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.119     |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.139     |
|    std                  | 0.0665     |
|    value_loss           | 0.0441     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | 7.78     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 123      |
|    time_elapsed    | 12488    |
|    total_timesteps | 1007616  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=18.91 +/- 2.12
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 18.9       |
| time/                   |            |
|    total timesteps      | 1010000    |
| train/                  |            |
|    approx_kl            | 0.10521391 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.5      |
|    explained_variance   | 0.946      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0855    |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.0662     |
|    value_loss           | 0.0546     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.14e+03 |
|    ep_rew_mean     | 8.28     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 124      |
|    time_elapsed    | 12630    |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=21.98 +/- 0.97
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 22         |
| time/                   |            |
|    total timesteps      | 1020000    |
| train/                  |            |
|    approx_kl            | 0.10076473 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.4      |
|    explained_variance   | 0.92       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.146     |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.135     |
|    std                  | 0.0658     |
|    value_loss           | 0.0579     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.16e+03 |
|    ep_rew_mean     | 8.51     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 125      |
|    time_elapsed    | 12761    |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=22.09 +/- 1.38
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 22.1        |
| time/                   |             |
|    total timesteps      | 1030000     |
| train/                  |             |
|    approx_kl            | 0.090534985 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.6       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.176      |
|    n_updates            | 2500        |
|    policy_gradient_loss | -0.131      |
|    std                  | 0.0656      |
|    value_loss           | 0.0348      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.17e+03 |
|    ep_rew_mean     | 8.7      |
| time/              |          |
|    fps             | 80       |
|    iterations      | 126      |
|    time_elapsed    | 12897    |
|    total_timesteps | 1032192  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=23.17 +/- 0.88
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 23.2        |
| time/                   |             |
|    total timesteps      | 1040000     |
| train/                  |             |
|    approx_kl            | 0.101323396 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.5       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.115      |
|    n_updates            | 2520        |
|    policy_gradient_loss | -0.135      |
|    std                  | 0.0653      |
|    value_loss           | 0.031       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.16e+03 |
|    ep_rew_mean     | 8.89     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 127      |
|    time_elapsed    | 13031    |
|    total_timesteps | 1040384  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.17e+03    |
|    ep_rew_mean          | 9.07        |
| time/                   |             |
|    fps                  | 80          |
|    iterations           | 128         |
|    time_elapsed         | 13105       |
|    total_timesteps      | 1048576     |
| train/                  |             |
|    approx_kl            | 0.097305246 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.5       |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.177      |
|    n_updates            | 2540        |
|    policy_gradient_loss | -0.13       |
|    std                  | 0.0651      |
|    value_loss           | 0.0426      |
-----------------------------------------
Eval num_timesteps=1050000, episode_reward=25.03 +/- 0.13
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 25          |
| time/                   |             |
|    total timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.094280556 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.4         |
|    entropy_loss         | -12.4       |
|    explained_variance   | 0.952       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.14       |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.126      |
|    std                  | 0.0649      |
|    value_loss           | 0.0317      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.2e+03  |
|    ep_rew_mean     | 9.58     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 129      |
|    time_elapsed    | 13248    |
|    total_timesteps | 1056768  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=25.28 +/- 0.30
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.3       |
| time/                   |            |
|    total timesteps      | 1060000    |
| train/                  |            |
|    approx_kl            | 0.09917027 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.3      |
|    explained_variance   | 0.943      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.156     |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.0646     |
|    value_loss           | 0.0604     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 79       |
|    iterations      | 130      |
|    time_elapsed    | 13388    |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=25.36 +/- 0.27
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total timesteps      | 1070000    |
| train/                  |            |
|    approx_kl            | 0.09782717 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.3      |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.154     |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.0643     |
|    value_loss           | 0.0533     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.3e+03  |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 131      |
|    time_elapsed    | 13530    |
|    total_timesteps | 1073152  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=25.54 +/- 0.20
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.5       |
| time/                   |            |
|    total timesteps      | 1080000    |
| train/                  |            |
|    approx_kl            | 0.09994128 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.2      |
|    explained_variance   | 0.95       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.179     |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.0641     |
|    value_loss           | 0.0511     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.33e+03 |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 132      |
|    time_elapsed    | 13671    |
|    total_timesteps | 1081344  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.35e+03   |
|    ep_rew_mean          | 11.9       |
| time/                   |            |
|    fps                  | 79         |
|    iterations           | 133        |
|    time_elapsed         | 13749      |
|    total_timesteps      | 1089536    |
| train/                  |            |
|    approx_kl            | 0.09185895 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.1      |
|    explained_variance   | 0.949      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0558    |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.0638     |
|    value_loss           | 0.039      |
----------------------------------------
Eval num_timesteps=1090000, episode_reward=25.61 +/- 0.18
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total timesteps      | 1090000    |
| train/                  |            |
|    approx_kl            | 0.09413855 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.1      |
|    explained_variance   | 0.941      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.174     |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.0636     |
|    value_loss           | 0.0383     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.37e+03 |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 134      |
|    time_elapsed    | 13891    |
|    total_timesteps | 1097728  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=25.50 +/- 0.15
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.5       |
| time/                   |            |
|    total timesteps      | 1100000    |
| train/                  |            |
|    approx_kl            | 0.10168353 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12.1      |
|    explained_variance   | 0.953      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0883    |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.0633     |
|    value_loss           | 0.0343     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.4e+03  |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 135      |
|    time_elapsed    | 14032    |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=24.90 +/- 0.13
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 24.9        |
| time/                   |             |
|    total timesteps      | 1110000     |
| train/                  |             |
|    approx_kl            | 0.094312824 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.4         |
|    entropy_loss         | -12         |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.193      |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.13       |
|    std                  | 0.063       |
|    value_loss           | 0.0413      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.41e+03 |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 78       |
|    iterations      | 136      |
|    time_elapsed    | 14175    |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=25.23 +/- 0.32
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total timesteps      | 1120000    |
| train/                  |            |
|    approx_kl            | 0.09645423 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.4        |
|    entropy_loss         | -12        |
|    explained_variance   | 0.946      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.181     |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.0628     |
|    value_loss           | 0.0621     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.42e+03 |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 137      |
|    time_elapsed    | 14313    |
|    total_timesteps | 1122304  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=24.08 +/- 0.42
Episode length: 1998.60 +/- 2.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 24.1       |
| time/                   |            |
|    total timesteps      | 1130000    |
| train/                  |            |
|    approx_kl            | 0.09925104 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12        |
|    explained_variance   | 0.942      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.165     |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.0625     |
|    value_loss           | 0.0277     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.42e+03 |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 138      |
|    time_elapsed    | 14452    |
|    total_timesteps | 1130496  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.46e+03   |
|    ep_rew_mean          | 13.9       |
| time/                   |            |
|    fps                  | 78         |
|    iterations           | 139        |
|    time_elapsed         | 14528      |
|    total_timesteps      | 1138688    |
| train/                  |            |
|    approx_kl            | 0.09570683 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.4        |
|    entropy_loss         | -12        |
|    explained_variance   | 0.954      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.147     |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.0623     |
|    value_loss           | 0.0296     |
----------------------------------------
Eval num_timesteps=1140000, episode_reward=24.73 +/- 0.13
Episode length: 1992.00 +/- 16.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total timesteps      | 1140000    |
| train/                  |            |
|    approx_kl            | 0.09108432 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.9      |
|    explained_variance   | 0.94       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.164     |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.0621     |
|    value_loss           | 0.0421     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.44e+03 |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 140      |
|    time_elapsed    | 14671    |
|    total_timesteps | 1146880  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=24.84 +/- 0.13
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 24.8       |
| time/                   |            |
|    total timesteps      | 1150000    |
| train/                  |            |
|    approx_kl            | 0.10629544 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.8      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.147     |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.0619     |
|    value_loss           | 0.0434     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.47e+03 |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 141      |
|    time_elapsed    | 14804    |
|    total_timesteps | 1155072  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=25.22 +/- 0.11
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total timesteps      | 1160000    |
| train/                  |            |
|    approx_kl            | 0.08977665 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.8      |
|    explained_variance   | 0.936      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0533    |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.0617     |
|    value_loss           | 0.045      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.47e+03 |
|    ep_rew_mean     | 14.5     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 142      |
|    time_elapsed    | 14944    |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=24.95 +/- 0.10
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 24.9       |
| time/                   |            |
|    total timesteps      | 1170000    |
| train/                  |            |
|    approx_kl            | 0.09793318 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.9      |
|    explained_variance   | 0.961      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.145     |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.0614     |
|    value_loss           | 0.0284     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.47e+03 |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 143      |
|    time_elapsed    | 15089    |
|    total_timesteps | 1171456  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.51e+03    |
|    ep_rew_mean          | 15.4        |
| time/                   |             |
|    fps                  | 77          |
|    iterations           | 144         |
|    time_elapsed         | 15166       |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.094491586 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.8       |
|    explained_variance   | 0.941       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.122      |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.112      |
|    std                  | 0.0612      |
|    value_loss           | 0.0381      |
-----------------------------------------
Eval num_timesteps=1180000, episode_reward=25.40 +/- 0.07
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 25.4        |
| time/                   |             |
|    total timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.094341606 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.7       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.159      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.126      |
|    std                  | 0.061       |
|    value_loss           | 0.0272      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.54e+03 |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 145      |
|    time_elapsed    | 15304    |
|    total_timesteps | 1187840  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=25.04 +/- 0.07
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 25          |
| time/                   |             |
|    total timesteps      | 1190000     |
| train/                  |             |
|    approx_kl            | 0.097806625 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.7       |
|    explained_variance   | 0.948       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.129      |
|    n_updates            | 2900        |
|    policy_gradient_loss | -0.125      |
|    std                  | 0.0608      |
|    value_loss           | 0.0303      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.56e+03 |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 146      |
|    time_elapsed    | 15447    |
|    total_timesteps | 1196032  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=25.06 +/- 0.16
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total timesteps      | 1200000    |
| train/                  |            |
|    approx_kl            | 0.09826931 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.6      |
|    explained_variance   | 0.943      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.168     |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.0607     |
|    value_loss           | 0.0353     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 147      |
|    time_elapsed    | 15589    |
|    total_timesteps | 1204224  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=25.37 +/- 0.09
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 25.4        |
| time/                   |             |
|    total timesteps      | 1210000     |
| train/                  |             |
|    approx_kl            | 0.098934166 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.6       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.17       |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.0604      |
|    value_loss           | 0.0352      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 148      |
|    time_elapsed    | 15733    |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=25.64 +/- 0.10
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total timesteps      | 1220000    |
| train/                  |            |
|    approx_kl            | 0.10225649 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.6      |
|    explained_variance   | 0.959      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.174     |
|    n_updates            | 2960       |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.0602     |
|    value_loss           | 0.0283     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 149      |
|    time_elapsed    | 15869    |
|    total_timesteps | 1220608  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.56e+03   |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 77         |
|    iterations           | 150        |
|    time_elapsed         | 15945      |
|    total_timesteps      | 1228800    |
| train/                  |            |
|    approx_kl            | 0.09865716 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.5      |
|    explained_variance   | 0.949      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0849    |
|    n_updates            | 2980       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.06       |
|    value_loss           | 0.0265     |
----------------------------------------
Eval num_timesteps=1230000, episode_reward=25.96 +/- 0.19
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26         |
| time/                   |            |
|    total timesteps      | 1230000    |
| train/                  |            |
|    approx_kl            | 0.10103229 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.4      |
|    explained_variance   | 0.953      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.142     |
|    n_updates            | 3000       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.0598     |
|    value_loss           | 0.0312     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 151      |
|    time_elapsed    | 16091    |
|    total_timesteps | 1236992  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=26.08 +/- 0.22
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total timesteps      | 1240000    |
| train/                  |            |
|    approx_kl            | 0.09656355 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.5      |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.14      |
|    n_updates            | 3020       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.0595     |
|    value_loss           | 0.0353     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.54e+03 |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 152      |
|    time_elapsed    | 16233    |
|    total_timesteps | 1245184  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=25.74 +/- 0.23
Episode length: 1991.20 +/- 17.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total timesteps      | 1250000    |
| train/                  |            |
|    approx_kl            | 0.09328533 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.5      |
|    explained_variance   | 0.944      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.129     |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.0593     |
|    value_loss           | 0.0265     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.54e+03 |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 153      |
|    time_elapsed    | 16369    |
|    total_timesteps | 1253376  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=25.95 +/- 0.06
Episode length: 1999.00 +/- 2.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26          |
| time/                   |             |
|    total timesteps      | 1260000     |
| train/                  |             |
|    approx_kl            | 0.096307665 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.4       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-05       |
|    loss                 | -0.124      |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.115      |
|    std                  | 0.0591      |
|    value_loss           | 0.0217      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 154      |
|    time_elapsed    | 16513    |
|    total_timesteps | 1261568  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.57e+03    |
|    ep_rew_mean          | 17.1        |
| time/                   |             |
|    fps                  | 76          |
|    iterations           | 155         |
|    time_elapsed         | 16590       |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.103599474 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.2       |
|    explained_variance   | 0.951       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.138      |
|    n_updates            | 3080        |
|    policy_gradient_loss | -0.128      |
|    std                  | 0.0588      |
|    value_loss           | 0.0205      |
-----------------------------------------
Eval num_timesteps=1270000, episode_reward=26.42 +/- 0.04
Episode length: 1994.60 +/- 10.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total timesteps      | 1270000    |
| train/                  |            |
|    approx_kl            | 0.10862443 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.16      |
|    n_updates            | 3100       |
|    policy_gradient_loss | -0.124     |
|    std                  | 0.0586     |
|    value_loss           | 0.0331     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 156      |
|    time_elapsed    | 16730    |
|    total_timesteps | 1277952  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=25.99 +/- 0.18
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26          |
| time/                   |             |
|    total timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.096445546 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.3       |
|    explained_variance   | 0.945       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.121      |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.115      |
|    std                  | 0.0584      |
|    value_loss           | 0.0257      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 157      |
|    time_elapsed    | 16870    |
|    total_timesteps | 1286144  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=25.96 +/- 0.08
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26         |
| time/                   |            |
|    total timesteps      | 1290000    |
| train/                  |            |
|    approx_kl            | 0.09534715 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.961      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.163     |
|    n_updates            | 3140       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.0583     |
|    value_loss           | 0.0132     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 158      |
|    time_elapsed    | 17011    |
|    total_timesteps | 1294336  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=26.31 +/- 0.14
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.3       |
| time/                   |            |
|    total timesteps      | 1300000    |
| train/                  |            |
|    approx_kl            | 0.09662047 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.2      |
|    explained_variance   | 0.958      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.123     |
|    n_updates            | 3160       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.058      |
|    value_loss           | 0.0182     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 159      |
|    time_elapsed    | 17149    |
|    total_timesteps | 1302528  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=26.19 +/- 0.04
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.2       |
| time/                   |            |
|    total timesteps      | 1310000    |
| train/                  |            |
|    approx_kl            | 0.09882206 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11.1      |
|    explained_variance   | 0.946      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.139     |
|    n_updates            | 3180       |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.0578     |
|    value_loss           | 0.0214     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 160      |
|    time_elapsed    | 17284    |
|    total_timesteps | 1310720  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.55e+03  |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 75        |
|    iterations           | 161       |
|    time_elapsed         | 17361     |
|    total_timesteps      | 1318912   |
| train/                  |           |
|    approx_kl            | 0.0985567 |
|    clip_fraction        | 0.284     |
|    clip_range           | 0.4       |
|    entropy_loss         | -11       |
|    explained_variance   | 0.954     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.127    |
|    n_updates            | 3200      |
|    policy_gradient_loss | -0.115    |
|    std                  | 0.0576    |
|    value_loss           | 0.0203    |
---------------------------------------
Eval num_timesteps=1320000, episode_reward=26.15 +/- 0.02
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26.1        |
| time/                   |             |
|    total timesteps      | 1320000     |
| train/                  |             |
|    approx_kl            | 0.100071505 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.929       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.154      |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.119      |
|    std                  | 0.0575      |
|    value_loss           | 0.029       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 162      |
|    time_elapsed    | 17500    |
|    total_timesteps | 1327104  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=26.22 +/- 0.06
Episode length: 1981.20 +/- 30.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.98e+03   |
|    mean_reward          | 26.2       |
| time/                   |            |
|    total timesteps      | 1330000    |
| train/                  |            |
|    approx_kl            | 0.09520081 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -11        |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.14      |
|    n_updates            | 3240       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.0573     |
|    value_loss           | 0.0237     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.56e+03 |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 163      |
|    time_elapsed    | 17641    |
|    total_timesteps | 1335296  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=26.56 +/- 0.09
Episode length: 1998.00 +/- 4.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26.6        |
| time/                   |             |
|    total timesteps      | 1340000     |
| train/                  |             |
|    approx_kl            | 0.099421784 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.127      |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.117      |
|    std                  | 0.0572      |
|    value_loss           | 0.0213      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 164      |
|    time_elapsed    | 17780    |
|    total_timesteps | 1343488  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=26.49 +/- 0.06
Episode length: 1997.20 +/- 5.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26.5        |
| time/                   |             |
|    total timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.096027195 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.4         |
|    entropy_loss         | -11.1       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.151      |
|    n_updates            | 3280        |
|    policy_gradient_loss | -0.115      |
|    std                  | 0.057       |
|    value_loss           | 0.0191      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.56e+03 |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 165      |
|    time_elapsed    | 17919    |
|    total_timesteps | 1351680  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.56e+03 |
|    ep_rew_mean          | 17.9     |
| time/                   |          |
|    fps                  | 75       |
|    iterations           | 166      |
|    time_elapsed         | 17995    |
|    total_timesteps      | 1359872  |
| train/                  |          |
|    approx_kl            | 0.097268 |
|    clip_fraction        | 0.28     |
|    clip_range           | 0.4      |
|    entropy_loss         | -10.9    |
|    explained_variance   | 0.936    |
|    learning_rate        | 3e-05    |
|    loss                 | -0.161   |
|    n_updates            | 3300     |
|    policy_gradient_loss | -0.118   |
|    std                  | 0.0569   |
|    value_loss           | 0.0312   |
--------------------------------------
Eval num_timesteps=1360000, episode_reward=26.44 +/- 0.08
Episode length: 1994.20 +/- 11.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total timesteps      | 1360000    |
| train/                  |            |
|    approx_kl            | 0.09629045 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.962      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.111     |
|    n_updates            | 3320       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.0567     |
|    value_loss           | 0.0125     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 75       |
|    iterations      | 167      |
|    time_elapsed    | 18132    |
|    total_timesteps | 1368064  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=26.50 +/- 0.06
Episode length: 1996.80 +/- 6.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26.5        |
| time/                   |             |
|    total timesteps      | 1370000     |
| train/                  |             |
|    approx_kl            | 0.102541566 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.4         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.936       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.135      |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.118      |
|    std                  | 0.0565      |
|    value_loss           | 0.0356      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 168      |
|    time_elapsed    | 18267    |
|    total_timesteps | 1376256  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=26.66 +/- 0.12
Episode length: 2000.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26.7        |
| time/                   |             |
|    total timesteps      | 1380000     |
| train/                  |             |
|    approx_kl            | 0.099130824 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.4         |
|    entropy_loss         | -11         |
|    explained_variance   | 0.964       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.154      |
|    n_updates            | 3360        |
|    policy_gradient_loss | -0.124      |
|    std                  | 0.0563      |
|    value_loss           | 0.0121      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 169      |
|    time_elapsed    | 18406    |
|    total_timesteps | 1384448  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=26.53 +/- 0.12
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.5       |
| time/                   |            |
|    total timesteps      | 1390000    |
| train/                  |            |
|    approx_kl            | 0.09226066 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.9      |
|    explained_variance   | 0.933      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.125     |
|    n_updates            | 3380       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.0561     |
|    value_loss           | 0.0196     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 170      |
|    time_elapsed    | 18534    |
|    total_timesteps | 1392640  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=26.21 +/- 0.09
Episode length: 1997.40 +/- 5.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2e+03       |
|    mean_reward          | 26.2        |
| time/                   |             |
|    total timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.105416015 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.4         |
|    entropy_loss         | -10.9       |
|    explained_variance   | 0.95        |
|    learning_rate        | 3e-05       |
|    loss                 | -0.143      |
|    n_updates            | 3400        |
|    policy_gradient_loss | -0.111      |
|    std                  | 0.0559      |
|    value_loss           | 0.0185      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 171      |
|    time_elapsed    | 18663    |
|    total_timesteps | 1400832  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.59e+03    |
|    ep_rew_mean          | 18.7        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 172         |
|    time_elapsed         | 18734       |
|    total_timesteps      | 1409024     |
| train/                  |             |
|    approx_kl            | 0.104546584 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.4         |
|    entropy_loss         | -10.8       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.151      |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.123      |
|    std                  | 0.0557      |
|    value_loss           | 0.0198      |
-----------------------------------------
Eval num_timesteps=1410000, episode_reward=26.02 +/- 0.17
Episode length: 1990.80 +/- 13.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 26         |
| time/                   |            |
|    total timesteps      | 1410000    |
| train/                  |            |
|    approx_kl            | 0.10052881 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.935      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.154     |
|    n_updates            | 3440       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.0555     |
|    value_loss           | 0.0198     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 173      |
|    time_elapsed    | 18865    |
|    total_timesteps | 1417216  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=26.04 +/- 0.12
Episode length: 1987.80 +/- 15.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 26         |
| time/                   |            |
|    total timesteps      | 1420000    |
| train/                  |            |
|    approx_kl            | 0.09025336 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.8      |
|    explained_variance   | 0.938      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.135     |
|    n_updates            | 3460       |
|    policy_gradient_loss | -0.105     |
|    std                  | 0.0553     |
|    value_loss           | 0.0207     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 174      |
|    time_elapsed    | 18994    |
|    total_timesteps | 1425408  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=26.31 +/- 0.06
Episode length: 1994.20 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.99e+03  |
|    mean_reward          | 26.3      |
| time/                   |           |
|    total timesteps      | 1430000   |
| train/                  |           |
|    approx_kl            | 0.0932716 |
|    clip_fraction        | 0.259     |
|    clip_range           | 0.4       |
|    entropy_loss         | -10.7     |
|    explained_variance   | 0.96      |
|    learning_rate        | 3e-05     |
|    loss                 | -0.163    |
|    n_updates            | 3480      |
|    policy_gradient_loss | -0.112    |
|    std                  | 0.0551    |
|    value_loss           | 0.00787   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 175      |
|    time_elapsed    | 19122    |
|    total_timesteps | 1433600  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=26.09 +/- 0.09
Episode length: 1988.40 +/- 23.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total timesteps      | 1440000    |
| train/                  |            |
|    approx_kl            | 0.09948103 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.943      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.145     |
|    n_updates            | 3500       |
|    policy_gradient_loss | -0.115     |
|    std                  | 0.0549     |
|    value_loss           | 0.0286     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 176      |
|    time_elapsed    | 19251    |
|    total_timesteps | 1441792  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.57e+03   |
|    ep_rew_mean          | 18.9       |
| time/                   |            |
|    fps                  | 75         |
|    iterations           | 177        |
|    time_elapsed         | 19325      |
|    total_timesteps      | 1449984    |
| train/                  |            |
|    approx_kl            | 0.10625747 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.914      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.175     |
|    n_updates            | 3520       |
|    policy_gradient_loss | -0.131     |
|    std                  | 0.0547     |
|    value_loss           | 0.0201     |
----------------------------------------
Eval num_timesteps=1450000, episode_reward=26.26 +/- 0.04
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.3       |
| time/                   |            |
|    total timesteps      | 1450000    |
| train/                  |            |
|    approx_kl            | 0.10312708 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.7      |
|    explained_variance   | 0.942      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.165     |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.0545     |
|    value_loss           | 0.0139     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.54e+03 |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 178      |
|    time_elapsed    | 19459    |
|    total_timesteps | 1458176  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=26.59 +/- 0.06
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.6       |
| time/                   |            |
|    total timesteps      | 1460000    |
| train/                  |            |
|    approx_kl            | 0.09785449 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.6      |
|    explained_variance   | 0.955      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.155     |
|    n_updates            | 3560       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.0543     |
|    value_loss           | 0.0105     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.53e+03 |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 179      |
|    time_elapsed    | 19592    |
|    total_timesteps | 1466368  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=26.61 +/- 0.14
Episode length: 1982.00 +/- 22.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.98e+03   |
|    mean_reward          | 26.6       |
| time/                   |            |
|    total timesteps      | 1470000    |
| train/                  |            |
|    approx_kl            | 0.09803897 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.121     |
|    n_updates            | 3580       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.0541     |
|    value_loss           | 0.0176     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 180      |
|    time_elapsed    | 19725    |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=26.91 +/- 0.16
Episode length: 1995.60 +/- 8.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.9       |
| time/                   |            |
|    total timesteps      | 1480000    |
| train/                  |            |
|    approx_kl            | 0.10191533 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.5      |
|    explained_variance   | 0.94       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.129     |
|    n_updates            | 3600       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.054      |
|    value_loss           | 0.0177     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.53e+03 |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 181      |
|    time_elapsed    | 19859    |
|    total_timesteps | 1482752  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=26.63 +/- 0.06
Episode length: 1993.00 +/- 14.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 26.6       |
| time/                   |            |
|    total timesteps      | 1490000    |
| train/                  |            |
|    approx_kl            | 0.09666091 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.947      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.148     |
|    n_updates            | 3620       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.0538     |
|    value_loss           | 0.0125     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.54e+03 |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 182      |
|    time_elapsed    | 19997    |
|    total_timesteps | 1490944  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.55e+03   |
|    ep_rew_mean          | 19         |
| time/                   |            |
|    fps                  | 74         |
|    iterations           | 183        |
|    time_elapsed         | 20075      |
|    total_timesteps      | 1499136    |
| train/                  |            |
|    approx_kl            | 0.09542725 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.4      |
|    explained_variance   | 0.956      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.155     |
|    n_updates            | 3640       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.0536     |
|    value_loss           | 0.0109     |
----------------------------------------
Eval num_timesteps=1500000, episode_reward=26.04 +/- 0.05
Episode length: 1977.80 +/- 21.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.98e+03   |
|    mean_reward          | 26         |
| time/                   |            |
|    total timesteps      | 1500000    |
| train/                  |            |
|    approx_kl            | 0.10520509 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.952      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0896    |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.0534     |
|    value_loss           | 0.0178     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.56e+03 |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 184      |
|    time_elapsed    | 20211    |
|    total_timesteps | 1507328  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=25.88 +/- 0.18
Episode length: 1987.20 +/- 25.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.99e+03   |
|    mean_reward          | 25.9       |
| time/                   |            |
|    total timesteps      | 1510000    |
| train/                  |            |
|    approx_kl            | 0.11030169 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.3      |
|    explained_variance   | 0.939      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.176     |
|    n_updates            | 3680       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.0533     |
|    value_loss           | 0.0217     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 74       |
|    iterations      | 185      |
|    time_elapsed    | 20350    |
|    total_timesteps | 1515520  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=24.93 +/- 0.22
Episode length: 1987.20 +/- 25.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.99e+03    |
|    mean_reward          | 24.9        |
| time/                   |             |
|    total timesteps      | 1520000     |
| train/                  |             |
|    approx_kl            | 0.100522816 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.4         |
|    entropy_loss         | -10.2       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.142      |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.12       |
|    std                  | 0.0531      |
|    value_loss           | 0.017       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 74       |
|    iterations      | 186      |
|    time_elapsed    | 20489    |
|    total_timesteps | 1523712  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=25.59 +/- 0.17
Episode length: 1946.60 +/- 31.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.95e+03   |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total timesteps      | 1530000    |
| train/                  |            |
|    approx_kl            | 0.10991104 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.2      |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.094     |
|    n_updates            | 3720       |
|    policy_gradient_loss | -0.116     |
|    std                  | 0.053      |
|    value_loss           | 0.0158     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 187      |
|    time_elapsed    | 20625    |
|    total_timesteps | 1531904  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=25.87 +/- 0.14
Episode length: 1959.20 +/- 33.71
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.96e+03 |
|    mean_reward          | 25.9     |
| time/                   |          |
|    total timesteps      | 1540000  |
| train/                  |          |
|    approx_kl            | 0.094705 |
|    clip_fraction        | 0.26     |
|    clip_range           | 0.4      |
|    entropy_loss         | -10.1    |
|    explained_variance   | 0.946    |
|    learning_rate        | 3e-05    |
|    loss                 | -0.121   |
|    n_updates            | 3740     |
|    policy_gradient_loss | -0.106   |
|    std                  | 0.0528   |
|    value_loss           | 0.0108   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 188      |
|    time_elapsed    | 20759    |
|    total_timesteps | 1540096  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.59e+03    |
|    ep_rew_mean          | 19.8        |
| time/                   |             |
|    fps                  | 74          |
|    iterations           | 189         |
|    time_elapsed         | 20831       |
|    total_timesteps      | 1548288     |
| train/                  |             |
|    approx_kl            | 0.101452276 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.4         |
|    entropy_loss         | -10.1       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.145      |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.111      |
|    std                  | 0.0526      |
|    value_loss           | 0.019       |
-----------------------------------------
Eval num_timesteps=1550000, episode_reward=26.52 +/- 0.12
Episode length: 1962.40 +/- 34.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.96e+03   |
|    mean_reward          | 26.5       |
| time/                   |            |
|    total timesteps      | 1550000    |
| train/                  |            |
|    approx_kl            | 0.08728378 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.4        |
|    entropy_loss         | -10.1      |
|    explained_variance   | 0.941      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.155     |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.106     |
|    std                  | 0.0525     |
|    value_loss           | 0.0171     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 190      |
|    time_elapsed    | 20961    |
|    total_timesteps | 1556480  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=26.27 +/- 0.21
Episode length: 1935.20 +/- 33.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.94e+03    |
|    mean_reward          | 26.3        |
| time/                   |             |
|    total timesteps      | 1560000     |
| train/                  |             |
|    approx_kl            | 0.100572735 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.4         |
|    entropy_loss         | -10         |
|    explained_variance   | 0.944       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.146      |
|    n_updates            | 3800        |
|    policy_gradient_loss | -0.118      |
|    std                  | 0.0523      |
|    value_loss           | 0.011       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 191      |
|    time_elapsed    | 21089    |
|    total_timesteps | 1564672  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=26.06 +/- 0.07
Episode length: 1976.60 +/- 31.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.98e+03   |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total timesteps      | 1570000    |
| train/                  |            |
|    approx_kl            | 0.09825091 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.96      |
|    explained_variance   | 0.941      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.146     |
|    n_updates            | 3820       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.0522     |
|    value_loss           | 0.0133     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | 20.2     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 192      |
|    time_elapsed    | 21218    |
|    total_timesteps | 1572864  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=26.20 +/- 0.11
Episode length: 1942.00 +/- 4.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.94e+03   |
|    mean_reward          | 26.2       |
| time/                   |            |
|    total timesteps      | 1580000    |
| train/                  |            |
|    approx_kl            | 0.10469471 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.93      |
|    explained_variance   | 0.961      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.169     |
|    n_updates            | 3840       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.0521     |
|    value_loss           | 0.0136     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 20.5     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 193      |
|    time_elapsed    | 21343    |
|    total_timesteps | 1581056  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.64e+03   |
|    ep_rew_mean          | 20.7       |
| time/                   |            |
|    fps                  | 74         |
|    iterations           | 194        |
|    time_elapsed         | 21414      |
|    total_timesteps      | 1589248    |
| train/                  |            |
|    approx_kl            | 0.11233002 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.84      |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.125     |
|    n_updates            | 3860       |
|    policy_gradient_loss | -0.117     |
|    std                  | 0.052      |
|    value_loss           | 0.0177     |
----------------------------------------
Eval num_timesteps=1590000, episode_reward=25.18 +/- 0.32
Episode length: 1948.00 +/- 28.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.95e+03   |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total timesteps      | 1590000    |
| train/                  |            |
|    approx_kl            | 0.10033315 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.86      |
|    explained_variance   | 0.948      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.126     |
|    n_updates            | 3880       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.0519     |
|    value_loss           | 0.0139     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 195      |
|    time_elapsed    | 21544    |
|    total_timesteps | 1597440  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=24.86 +/- 0.16
Episode length: 1864.80 +/- 16.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.86e+03   |
|    mean_reward          | 24.9       |
| time/                   |            |
|    total timesteps      | 1600000    |
| train/                  |            |
|    approx_kl            | 0.10093675 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.65      |
|    explained_variance   | 0.932      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.106     |
|    n_updates            | 3900       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.0518     |
|    value_loss           | 0.0118     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 196      |
|    time_elapsed    | 21667    |
|    total_timesteps | 1605632  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=24.69 +/- 0.45
Episode length: 1929.00 +/- 58.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.93e+03    |
|    mean_reward          | 24.7        |
| time/                   |             |
|    total timesteps      | 1610000     |
| train/                  |             |
|    approx_kl            | 0.099979624 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.4         |
|    entropy_loss         | -9.74       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.155      |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.115      |
|    std                  | 0.0517      |
|    value_loss           | 0.0134      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 197      |
|    time_elapsed    | 21791    |
|    total_timesteps | 1613824  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=24.67 +/- 0.16
Episode length: 1901.20 +/- 21.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.9e+03     |
|    mean_reward          | 24.7        |
| time/                   |             |
|    total timesteps      | 1620000     |
| train/                  |             |
|    approx_kl            | 0.111470304 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.4         |
|    entropy_loss         | -9.67       |
|    explained_variance   | 0.931       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.154      |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.122      |
|    std                  | 0.0516      |
|    value_loss           | 0.0134      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 198      |
|    time_elapsed    | 21918    |
|    total_timesteps | 1622016  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=24.57 +/- 0.10
Episode length: 1874.80 +/- 14.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.87e+03   |
|    mean_reward          | 24.6       |
| time/                   |            |
|    total timesteps      | 1630000    |
| train/                  |            |
|    approx_kl            | 0.10666886 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.57      |
|    explained_variance   | 0.943      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.145     |
|    n_updates            | 3960       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.0513     |
|    value_loss           | 0.00941    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | 21.3     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 199      |
|    time_elapsed    | 22045    |
|    total_timesteps | 1630208  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.66e+03   |
|    ep_rew_mean          | 21.4       |
| time/                   |            |
|    fps                  | 74         |
|    iterations           | 200        |
|    time_elapsed         | 22117      |
|    total_timesteps      | 1638400    |
| train/                  |            |
|    approx_kl            | 0.09312768 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.68      |
|    explained_variance   | 0.944      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.138     |
|    n_updates            | 3980       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.0511     |
|    value_loss           | 0.0113     |
----------------------------------------
Eval num_timesteps=1640000, episode_reward=24.69 +/- 0.20
Episode length: 1914.00 +/- 33.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.91e+03   |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total timesteps      | 1640000    |
| train/                  |            |
|    approx_kl            | 0.10216124 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.53      |
|    explained_variance   | 0.945      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.153     |
|    n_updates            | 4000       |
|    policy_gradient_loss | -0.113     |
|    std                  | 0.051      |
|    value_loss           | 0.00932    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 74       |
|    iterations      | 201      |
|    time_elapsed    | 22245    |
|    total_timesteps | 1646592  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=24.79 +/- 0.09
Episode length: 1911.60 +/- 21.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.91e+03    |
|    mean_reward          | 24.8        |
| time/                   |             |
|    total timesteps      | 1650000     |
| train/                  |             |
|    approx_kl            | 0.099749565 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.4         |
|    entropy_loss         | -9.44       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.106      |
|    n_updates            | 4020        |
|    policy_gradient_loss | -0.106      |
|    std                  | 0.0509      |
|    value_loss           | 0.0111      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 202      |
|    time_elapsed    | 22374    |
|    total_timesteps | 1654784  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=25.21 +/- 0.21
Episode length: 1892.80 +/- 20.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.89e+03  |
|    mean_reward          | 25.2      |
| time/                   |           |
|    total timesteps      | 1660000   |
| train/                  |           |
|    approx_kl            | 0.1066882 |
|    clip_fraction        | 0.293     |
|    clip_range           | 0.4       |
|    entropy_loss         | -9.47     |
|    explained_variance   | 0.94      |
|    learning_rate        | 3e-05     |
|    loss                 | -0.125    |
|    n_updates            | 4040      |
|    policy_gradient_loss | -0.117    |
|    std                  | 0.0508    |
|    value_loss           | 0.011     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.68e+03 |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 203      |
|    time_elapsed    | 22500    |
|    total_timesteps | 1662976  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=25.09 +/- 0.14
Episode length: 1875.40 +/- 15.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.88e+03   |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total timesteps      | 1670000    |
| train/                  |            |
|    approx_kl            | 0.09526247 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.48      |
|    explained_variance   | 0.944      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.161     |
|    n_updates            | 4060       |
|    policy_gradient_loss | -0.108     |
|    std                  | 0.0506     |
|    value_loss           | 0.0135     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 204      |
|    time_elapsed    | 22627    |
|    total_timesteps | 1671168  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.62e+03   |
|    ep_rew_mean          | 21         |
| time/                   |            |
|    fps                  | 73         |
|    iterations           | 205        |
|    time_elapsed         | 22700      |
|    total_timesteps      | 1679360    |
| train/                  |            |
|    approx_kl            | 0.11128116 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.45      |
|    explained_variance   | 0.934      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.163     |
|    n_updates            | 4080       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.0505     |
|    value_loss           | 0.0181     |
----------------------------------------
Eval num_timesteps=1680000, episode_reward=25.50 +/- 0.06
Episode length: 1895.20 +/- 7.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.9e+03    |
|    mean_reward          | 25.5       |
| time/                   |            |
|    total timesteps      | 1680000    |
| train/                  |            |
|    approx_kl            | 0.09909022 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.43      |
|    explained_variance   | 0.942      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.136     |
|    n_updates            | 4100       |
|    policy_gradient_loss | -0.114     |
|    std                  | 0.0503     |
|    value_loss           | 0.00916    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 73       |
|    iterations      | 206      |
|    time_elapsed    | 22829    |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=25.68 +/- 0.06
Episode length: 1891.40 +/- 8.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.89e+03    |
|    mean_reward          | 25.7        |
| time/                   |             |
|    total timesteps      | 1690000     |
| train/                  |             |
|    approx_kl            | 0.107890345 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.4         |
|    entropy_loss         | -9.48       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.168      |
|    n_updates            | 4120        |
|    policy_gradient_loss | -0.119      |
|    std                  | 0.0501      |
|    value_loss           | 0.0121      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 207      |
|    time_elapsed    | 22957    |
|    total_timesteps | 1695744  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=25.87 +/- 0.08
Episode length: 1929.80 +/- 13.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.93e+03   |
|    mean_reward          | 25.9       |
| time/                   |            |
|    total timesteps      | 1700000    |
| train/                  |            |
|    approx_kl            | 0.11073063 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.33      |
|    explained_variance   | 0.941      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.135     |
|    n_updates            | 4140       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.05       |
|    value_loss           | 0.00802    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 208      |
|    time_elapsed    | 23084    |
|    total_timesteps | 1703936  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=26.35 +/- 0.11
Episode length: 1946.80 +/- 23.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.95e+03  |
|    mean_reward          | 26.3      |
| time/                   |           |
|    total timesteps      | 1710000   |
| train/                  |           |
|    approx_kl            | 0.1096714 |
|    clip_fraction        | 0.309     |
|    clip_range           | 0.4       |
|    entropy_loss         | -9.44     |
|    explained_variance   | 0.952     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.143    |
|    n_updates            | 4160      |
|    policy_gradient_loss | -0.128    |
|    std                  | 0.0499    |
|    value_loss           | 0.0149    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.6e+03  |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 209      |
|    time_elapsed    | 23213    |
|    total_timesteps | 1712128  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=26.71 +/- 0.04
Episode length: 1972.80 +/- 28.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.97e+03    |
|    mean_reward          | 26.7        |
| time/                   |             |
|    total timesteps      | 1720000     |
| train/                  |             |
|    approx_kl            | 0.109647736 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.4         |
|    entropy_loss         | -9.36       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.0999     |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.123      |
|    std                  | 0.0497      |
|    value_loss           | 0.0105      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 210      |
|    time_elapsed    | 23346    |
|    total_timesteps | 1720320  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.56e+03   |
|    ep_rew_mean          | 20.3       |
| time/                   |            |
|    fps                  | 73         |
|    iterations           | 211        |
|    time_elapsed         | 23422      |
|    total_timesteps      | 1728512    |
| train/                  |            |
|    approx_kl            | 0.09898034 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.35      |
|    explained_variance   | 0.935      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.165     |
|    n_updates            | 4200       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.0496     |
|    value_loss           | 0.0135     |
----------------------------------------
Eval num_timesteps=1730000, episode_reward=26.32 +/- 0.18
Episode length: 1942.80 +/- 30.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.94e+03   |
|    mean_reward          | 26.3       |
| time/                   |            |
|    total timesteps      | 1730000    |
| train/                  |            |
|    approx_kl            | 0.11007828 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.13      |
|    explained_variance   | 0.926      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.16      |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.122     |
|    std                  | 0.0495     |
|    value_loss           | 0.00978    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.55e+03 |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 212      |
|    time_elapsed    | 23556    |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=26.67 +/- 0.09
Episode length: 1997.00 +/- 6.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 26.7       |
| time/                   |            |
|    total timesteps      | 1740000    |
| train/                  |            |
|    approx_kl            | 0.10748935 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.21      |
|    explained_variance   | 0.953      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.152     |
|    n_updates            | 4240       |
|    policy_gradient_loss | -0.121     |
|    std                  | 0.0493     |
|    value_loss           | 0.00802    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.53e+03 |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 213      |
|    time_elapsed    | 23696    |
|    total_timesteps | 1744896  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=26.78 +/- 0.08
Episode length: 1974.00 +/- 23.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.97e+03  |
|    mean_reward          | 26.8      |
| time/                   |           |
|    total timesteps      | 1750000   |
| train/                  |           |
|    approx_kl            | 0.1025253 |
|    clip_fraction        | 0.291     |
|    clip_range           | 0.4       |
|    entropy_loss         | -9.25     |
|    explained_variance   | 0.943     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.151    |
|    n_updates            | 4260      |
|    policy_gradient_loss | -0.117    |
|    std                  | 0.0491    |
|    value_loss           | 0.00879   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.53e+03 |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 214      |
|    time_elapsed    | 23835    |
|    total_timesteps | 1753088  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=26.80 +/- 0.08
Episode length: 1992.00 +/- 9.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.99e+03    |
|    mean_reward          | 26.8        |
| time/                   |             |
|    total timesteps      | 1760000     |
| train/                  |             |
|    approx_kl            | 0.102004424 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.4         |
|    entropy_loss         | -9.22       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.158      |
|    n_updates            | 4280        |
|    policy_gradient_loss | -0.114      |
|    std                  | 0.049       |
|    value_loss           | 0.00758     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.54e+03 |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 215      |
|    time_elapsed    | 23976    |
|    total_timesteps | 1761280  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.54e+03    |
|    ep_rew_mean          | 20.1        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 216         |
|    time_elapsed         | 24050       |
|    total_timesteps      | 1769472     |
| train/                  |             |
|    approx_kl            | 0.106366634 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.4         |
|    entropy_loss         | -9.09       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.134      |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.116      |
|    std                  | 0.0488      |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=1770000, episode_reward=26.87 +/- 0.06
Episode length: 1964.00 +/- 14.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.96e+03  |
|    mean_reward          | 26.9      |
| time/                   |           |
|    total timesteps      | 1770000   |
| train/                  |           |
|    approx_kl            | 0.1076772 |
|    clip_fraction        | 0.306     |
|    clip_range           | 0.4       |
|    entropy_loss         | -9        |
|    explained_variance   | 0.944     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.165    |
|    n_updates            | 4320      |
|    policy_gradient_loss | -0.126    |
|    std                  | 0.0487    |
|    value_loss           | 0.0118    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.54e+03 |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 217      |
|    time_elapsed    | 24184    |
|    total_timesteps | 1777664  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=27.15 +/- 0.21
Episode length: 1978.00 +/- 24.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.98e+03   |
|    mean_reward          | 27.1       |
| time/                   |            |
|    total timesteps      | 1780000    |
| train/                  |            |
|    approx_kl            | 0.12353133 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.05      |
|    explained_variance   | 0.944      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.135     |
|    n_updates            | 4340       |
|    policy_gradient_loss | -0.144     |
|    std                  | 0.0485     |
|    value_loss           | 0.0165     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 20.5     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 218      |
|    time_elapsed    | 24318    |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=27.35 +/- 0.20
Episode length: 1981.20 +/- 20.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.98e+03    |
|    mean_reward          | 27.3        |
| time/                   |             |
|    total timesteps      | 1790000     |
| train/                  |             |
|    approx_kl            | 0.103945106 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.94       |
|    explained_variance   | 0.938       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.146      |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.125      |
|    std                  | 0.0484      |
|    value_loss           | 0.00733     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.57e+03 |
|    ep_rew_mean     | 20.6     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 219      |
|    time_elapsed    | 24457    |
|    total_timesteps | 1794048  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=27.11 +/- 0.38
Episode length: 1965.80 +/- 41.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.97e+03   |
|    mean_reward          | 27.1       |
| time/                   |            |
|    total timesteps      | 1800000    |
| train/                  |            |
|    approx_kl            | 0.09762875 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.4        |
|    entropy_loss         | -9.01      |
|    explained_variance   | 0.957      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.138     |
|    n_updates            | 4380       |
|    policy_gradient_loss | -0.111     |
|    std                  | 0.0482     |
|    value_loss           | 0.00767    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.56e+03 |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 220      |
|    time_elapsed    | 24577    |
|    total_timesteps | 1802240  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=27.85 +/- 0.11
Episode length: 2000.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 2e+03      |
|    mean_reward          | 27.9       |
| time/                   |            |
|    total timesteps      | 1810000    |
| train/                  |            |
|    approx_kl            | 0.11436121 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.87      |
|    explained_variance   | 0.946      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.126     |
|    n_updates            | 4400       |
|    policy_gradient_loss | -0.128     |
|    std                  | 0.0481     |
|    value_loss           | 0.0117     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 221      |
|    time_elapsed    | 24702    |
|    total_timesteps | 1810432  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.59e+03  |
|    ep_rew_mean          | 20.7      |
| time/                   |           |
|    fps                  | 73        |
|    iterations           | 222       |
|    time_elapsed         | 24771     |
|    total_timesteps      | 1818624   |
| train/                  |           |
|    approx_kl            | 0.1128589 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.4       |
|    entropy_loss         | -8.91     |
|    explained_variance   | 0.954     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.181    |
|    n_updates            | 4420      |
|    policy_gradient_loss | -0.128    |
|    std                  | 0.048     |
|    value_loss           | 0.0108    |
---------------------------------------
Eval num_timesteps=1820000, episode_reward=27.17 +/- 0.36
Episode length: 1968.60 +/- 28.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.97e+03   |
|    mean_reward          | 27.2       |
| time/                   |            |
|    total timesteps      | 1820000    |
| train/                  |            |
|    approx_kl            | 0.12084687 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.89      |
|    explained_variance   | 0.952      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.15      |
|    n_updates            | 4440       |
|    policy_gradient_loss | -0.129     |
|    std                  | 0.0478     |
|    value_loss           | 0.00922    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.58e+03 |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 223      |
|    time_elapsed    | 24893    |
|    total_timesteps | 1826816  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=26.19 +/- 0.48
Episode length: 1903.80 +/- 28.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.9e+03    |
|    mean_reward          | 26.2       |
| time/                   |            |
|    total timesteps      | 1830000    |
| train/                  |            |
|    approx_kl            | 0.10284018 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.82      |
|    explained_variance   | 0.96       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.152     |
|    n_updates            | 4460       |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.0477     |
|    value_loss           | 0.00803    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 224      |
|    time_elapsed    | 25018    |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=26.81 +/- 0.50
Episode length: 1947.00 +/- 45.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.95e+03    |
|    mean_reward          | 26.8        |
| time/                   |             |
|    total timesteps      | 1840000     |
| train/                  |             |
|    approx_kl            | 0.112576164 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.71       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.167      |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.13       |
|    std                  | 0.0476      |
|    value_loss           | 0.00719     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 225      |
|    time_elapsed    | 25144    |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=25.63 +/- 0.09
Episode length: 1864.80 +/- 8.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.86e+03   |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total timesteps      | 1850000    |
| train/                  |            |
|    approx_kl            | 0.10970347 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.8       |
|    explained_variance   | 0.957      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.162     |
|    n_updates            | 4500       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.0474     |
|    value_loss           | 0.0113     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.61e+03 |
|    ep_rew_mean     | 21.2     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 226      |
|    time_elapsed    | 25263    |
|    total_timesteps | 1851392  |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.61e+03   |
|    ep_rew_mean          | 21.3       |
| time/                   |            |
|    fps                  | 73         |
|    iterations           | 227        |
|    time_elapsed         | 25333      |
|    total_timesteps      | 1859584    |
| train/                  |            |
|    approx_kl            | 0.10083302 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.6       |
|    explained_variance   | 0.959      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.115     |
|    n_updates            | 4520       |
|    policy_gradient_loss | -0.118     |
|    std                  | 0.0473     |
|    value_loss           | 0.00534    |
----------------------------------------
Eval num_timesteps=1860000, episode_reward=25.37 +/- 0.16
Episode length: 1860.00 +/- 12.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.86e+03  |
|    mean_reward          | 25.4      |
| time/                   |           |
|    total timesteps      | 1860000   |
| train/                  |           |
|    approx_kl            | 0.1193364 |
|    clip_fraction        | 0.317     |
|    clip_range           | 0.4       |
|    entropy_loss         | -8.58     |
|    explained_variance   | 0.951     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.0873   |
|    n_updates            | 4540      |
|    policy_gradient_loss | -0.119    |
|    std                  | 0.0472    |
|    value_loss           | 0.0133    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 228      |
|    time_elapsed    | 25449    |
|    total_timesteps | 1867776  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=25.14 +/- 0.09
Episode length: 1843.60 +/- 2.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.84e+03   |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total timesteps      | 1870000    |
| train/                  |            |
|    approx_kl            | 0.11629531 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.53      |
|    explained_variance   | 0.941      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.118     |
|    n_updates            | 4560       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.0471     |
|    value_loss           | 0.0154     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | 21.9     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 229      |
|    time_elapsed    | 25572    |
|    total_timesteps | 1875968  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=25.45 +/- 0.19
Episode length: 1846.00 +/- 19.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.85e+03   |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total timesteps      | 1880000    |
| train/                  |            |
|    approx_kl            | 0.11459103 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.69      |
|    explained_variance   | 0.937      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.133     |
|    n_updates            | 4580       |
|    policy_gradient_loss | -0.119     |
|    std                  | 0.0469     |
|    value_loss           | 0.0148     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 230      |
|    time_elapsed    | 25695    |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=25.57 +/- 0.19
Episode length: 1871.60 +/- 39.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.87e+03   |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total timesteps      | 1890000    |
| train/                  |            |
|    approx_kl            | 0.10482524 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.42      |
|    explained_variance   | 0.944      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.172     |
|    n_updates            | 4600       |
|    policy_gradient_loss | -0.12      |
|    std                  | 0.0468     |
|    value_loss           | 0.00645    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | 21.6     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 231      |
|    time_elapsed    | 25807    |
|    total_timesteps | 1892352  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=25.97 +/- 0.13
Episode length: 1881.80 +/- 9.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.88e+03  |
|    mean_reward          | 26        |
| time/                   |           |
|    total timesteps      | 1900000   |
| train/                  |           |
|    approx_kl            | 0.1131702 |
|    clip_fraction        | 0.301     |
|    clip_range           | 0.4       |
|    entropy_loss         | -8.46     |
|    explained_variance   | 0.947     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.15     |
|    n_updates            | 4620      |
|    policy_gradient_loss | -0.121    |
|    std                  | 0.0467    |
|    value_loss           | 0.0111    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 232      |
|    time_elapsed    | 25925    |
|    total_timesteps | 1900544  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.64e+03    |
|    ep_rew_mean          | 21.8        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 233         |
|    time_elapsed         | 25995       |
|    total_timesteps      | 1908736     |
| train/                  |             |
|    approx_kl            | 0.116270855 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.62       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.145      |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.12       |
|    std                  | 0.0465      |
|    value_loss           | 0.0181      |
-----------------------------------------
Eval num_timesteps=1910000, episode_reward=27.24 +/- 0.12
Episode length: 1959.60 +/- 13.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.96e+03   |
|    mean_reward          | 27.2       |
| time/                   |            |
|    total timesteps      | 1910000    |
| train/                  |            |
|    approx_kl            | 0.11037133 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.48      |
|    explained_variance   | 0.945      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.145     |
|    n_updates            | 4660       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.0464     |
|    value_loss           | 0.00764    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.65e+03 |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 73       |
|    iterations      | 234      |
|    time_elapsed    | 26117    |
|    total_timesteps | 1916928  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=26.59 +/- 0.19
Episode length: 1944.40 +/- 26.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.94e+03    |
|    mean_reward          | 26.6        |
| time/                   |             |
|    total timesteps      | 1920000     |
| train/                  |             |
|    approx_kl            | 0.105415754 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.32       |
|    explained_variance   | 0.949       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.115      |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.102      |
|    std                  | 0.0463      |
|    value_loss           | 0.00743     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.64e+03 |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 235      |
|    time_elapsed    | 26239    |
|    total_timesteps | 1925120  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=25.97 +/- 0.13
Episode length: 1885.60 +/- 23.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.89e+03    |
|    mean_reward          | 26          |
| time/                   |             |
|    total timesteps      | 1930000     |
| train/                  |             |
|    approx_kl            | 0.117102526 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.36       |
|    explained_variance   | 0.947       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.138      |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.119      |
|    std                  | 0.0462      |
|    value_loss           | 0.0079      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.66e+03 |
|    ep_rew_mean     | 22.1     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 236      |
|    time_elapsed    | 26354    |
|    total_timesteps | 1933312  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=26.09 +/- 0.14
Episode length: 1909.40 +/- 56.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.91e+03    |
|    mean_reward          | 26.1        |
| time/                   |             |
|    total timesteps      | 1940000     |
| train/                  |             |
|    approx_kl            | 0.119047366 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.935       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.167      |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.134      |
|    std                  | 0.0461      |
|    value_loss           | 0.0134      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | 22.4     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 237      |
|    time_elapsed    | 26475    |
|    total_timesteps | 1941504  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.66e+03    |
|    ep_rew_mean          | 22.3        |
| time/                   |             |
|    fps                  | 73          |
|    iterations           | 238         |
|    time_elapsed         | 26542       |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.101649866 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.38       |
|    explained_variance   | 0.957       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.113      |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.108      |
|    std                  | 0.0459      |
|    value_loss           | 0.00651     |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=26.54 +/- 0.04
Episode length: 1925.80 +/- 25.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.93e+03  |
|    mean_reward          | 26.5      |
| time/                   |           |
|    total timesteps      | 1950000   |
| train/                  |           |
|    approx_kl            | 0.1068486 |
|    clip_fraction        | 0.305     |
|    clip_range           | 0.4       |
|    entropy_loss         | -8.3      |
|    explained_variance   | 0.946     |
|    learning_rate        | 3e-05     |
|    loss                 | -0.152    |
|    n_updates            | 4760      |
|    policy_gradient_loss | -0.119    |
|    std                  | 0.0457    |
|    value_loss           | 0.00898   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | 22.4     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 239      |
|    time_elapsed    | 26665    |
|    total_timesteps | 1957888  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=26.48 +/- 0.19
Episode length: 1921.80 +/- 41.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.92e+03   |
|    mean_reward          | 26.5       |
| time/                   |            |
|    total timesteps      | 1960000    |
| train/                  |            |
|    approx_kl            | 0.11348097 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.29      |
|    explained_variance   | 0.963      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.179     |
|    n_updates            | 4780       |
|    policy_gradient_loss | -0.125     |
|    std                  | 0.0456     |
|    value_loss           | 0.00615    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.67e+03 |
|    ep_rew_mean     | 22.5     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 240      |
|    time_elapsed    | 29220    |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=26.12 +/- 0.12
Episode length: 1881.40 +/- 18.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.88e+03   |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total timesteps      | 1970000    |
| train/                  |            |
|    approx_kl            | 0.11344118 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.34      |
|    explained_variance   | 0.933      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.15      |
|    n_updates            | 4800       |
|    policy_gradient_loss | -0.126     |
|    std                  | 0.0455     |
|    value_loss           | 0.0115     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.63e+03 |
|    ep_rew_mean     | 21.9     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 241      |
|    time_elapsed    | 29358    |
|    total_timesteps | 1974272  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=26.19 +/- 0.23
Episode length: 1940.00 +/- 51.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.94e+03   |
|    mean_reward          | 26.2       |
| time/                   |            |
|    total timesteps      | 1980000    |
| train/                  |            |
|    approx_kl            | 0.09668006 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.17      |
|    explained_variance   | 0.922      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.168     |
|    n_updates            | 4820       |
|    policy_gradient_loss | -0.109     |
|    std                  | 0.0453     |
|    value_loss           | 0.0063     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 242      |
|    time_elapsed    | 29503    |
|    total_timesteps | 1982464  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=26.22 +/- 0.17
Episode length: 1940.00 +/- 43.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.94e+03   |
|    mean_reward          | 26.2       |
| time/                   |            |
|    total timesteps      | 1990000    |
| train/                  |            |
|    approx_kl            | 0.09896064 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.21      |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.152     |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.112     |
|    std                  | 0.0452     |
|    value_loss           | 0.00914    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.62e+03 |
|    ep_rew_mean     | 21.9     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 243      |
|    time_elapsed    | 29647    |
|    total_timesteps | 1990656  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.61e+03    |
|    ep_rew_mean          | 21.7        |
| time/                   |             |
|    fps                  | 67          |
|    iterations           | 244         |
|    time_elapsed         | 29725       |
|    total_timesteps      | 1998848     |
| train/                  |             |
|    approx_kl            | 0.110163584 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.11       |
|    explained_variance   | 0.942       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.17       |
|    n_updates            | 4860        |
|    policy_gradient_loss | -0.12       |
|    std                  | 0.0451      |
|    value_loss           | 0.0068      |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=26.00 +/- 0.28
Episode length: 1911.80 +/- 73.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.91e+03   |
|    mean_reward          | 26         |
| time/                   |            |
|    total timesteps      | 2000000    |
| train/                  |            |
|    approx_kl            | 0.11508157 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.3       |
|    explained_variance   | 0.951      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.168     |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.123     |
|    std                  | 0.0449     |
|    value_loss           | 0.0119     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.59e+03 |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 245      |
|    time_elapsed    | 29866    |
|    total_timesteps | 2007040  |
---------------------------------
Saving to logs/ppo/A1GymEnv-v0_119
